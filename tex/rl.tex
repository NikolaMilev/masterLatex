\chapter{Učenje potkrepljivanjem}
\label{ch:rl}

Učenje potkrepljivanjem je vid mašinskog učenja koji podseća na učenje koje se može naći u prirodi: ljudi i životinje uče na osnovu interakcije sa svetom oko sebe. Razlikovanje povoljnog i nepovoljnog ponašanja nije unapred učinjeno već je neophodno da onaj koji uči donese taj zaključak. U učenju potkrepljivanjem kao grani mašinskog učenja, komunikacija sa okruženjem svodi se na preduzimanje akcija u nekoj situaciji i dobijanja odgovora u vidu numeričke nagrade i informacije o tome kako se situacija promenila. Entitet koji komunicira sa svetom naziva se (softverskim) agentom dok se svet naziva okruženjem. Kao i kod ostalih oblasti mašinskog učenja, učenje potkrepljivanjem podrazumeva skup problema koji imaju za cilj obučavanje agenta radi maksimizacije dugoročne numeričke nagrade. 

\section{Osnovni pojmovi}
[MOŽDA SKLONI SECTION] \\
Učenje potkrepljivanjem sastoji se iz četiri komponente: politike ponašanja, nagrade, funkcije vrednosti i modela okruženja. Politika opisuje način na koji se agent ponaša. Nagrada predstavlja numerički signal koji agent dobija od okruženja. U toku učenja, cilj agenta je da maksimizuje ukupnu dobijenu nagradu od okruženja. Dakle, nagradom je implicitno objašnjeno šta je dobro a šta loše ponašanje. Prilikom rešavanja problema učenja potkrelpljivanjem, cilj je nalaženje optimalne politike, tj. politike čijim se praćenjem dobija najveća dugoročna nagrada. Funkcija vrednosti govori koliko je dobro naći se u nekom stanju okruženja. Za razliku od nagrade, funkcija vrednosti opisuje kvalitet nekog stanja na duže staze. Ova komponenta je neophodna jer je moguće da dolaskom u neko stanje agent dobije malu nagradu ali da samo stanje ima veliku vrednost, što znači da je dolaskom u stanje moguće dobiti veliku dugoročnu nagradu. Neki algoritmi učenja potkrepljivanjem koriste model okruženja kako bi planirali unapred. Moguće je koristit pristupe koji koriste model, pristupe koji ne koriste model već se uči iz iskustva, kao i pristupe koji koriste učenje iz iskustva radi učenja modela.
\par 
U učenju potkrepljivanjem javlja se potreba za uspostavljanjem balansa između istraživanja i iskorišćavanja već stečenog znanja (eng.~{\em exloration vs. exploitation}). Naime, na početku učenja, agent istražuje okruženje i time uči kako bi trebalo da se ponaša. Čak i kada se nauči neko ponašanje, neophodno je nastaviti sa istraživanjem, u nekoj meri. Kako već naučeno ponašanje možda nije najbolje, bez istraživanja je moguće završiti u nekom od lokalnih optimuma. Međutim, kako učenje teče i agent unapređuje svoje ponašanje, poželjno je pratiti politiku koja dovodi do velikih dugoročnih nagrada. Najčešće je stopa istraživanja velika na početku učenja i opada u toku ovog procesa. [OVO NEGDE DRUGDE?]

\section{Markovljevi procesi odlučivanja}
\label{sec:mdp}

Markovljevi procesi odlučivanja (eng. {\em Markov decision process}, skr. {\em MDP}) daju teorijski okvir u kome je moguće relativno jednostavno postaviti i rešiti problem učenja potkrepljivanjem. Markovljevi procesi odlučivanja opisuju okruženje s kojim je moguće komunicirati. Ta komunikacija sastoji se iz toga da se od okruženja može dobiti informacija o stanju i da se okruženju može poslati informacija o akciji koja se preduzima, na šta okruženje odgovara informacijama o novom stanju i o numeričkoj nagradi. Izuzetno je važno da se od okruženja ne može dobiti informacija o tome da li je preduzeta akcija prava ili ne već samo informacija o numeričkoj nagradi, koju na duže staze treba maksimizovati. Kako se pri učenju potkrepljivanjem pretpostavlja postojanje agenta, kao i postojanje okruženja, nadalje se podrazumeva da postoji neki agent koji komunicira sa okruženjem. Treba imati u vidu da Markovljevi procesi odlučivanja opisuju idealno okruženje. U praksi okruženja često nisu idealna i tada se pribegava metodima koji ne koriste MDP. Iako je veliki deo teorije u učenju potkrepljivanjem ograničen na MDP, iste ideje imaju širu primenu.
\par 
U nastavku je dat teorijski opis Markovljevih procesa odlučivanja dok će proces učenja biti opisan nešto kasnije, u [DEO KOJI OPISUJE UČENJE AGENTA].

%Pretpostavlja se da postoji agent koji komunicira sa okruženjem. Agent u svakom trenutku ima informaciju o %stanju okruženja i u mogućnosti je da preduzme akciju na šta od okruženja dobija odgovor u vidu novog stanja %i numeričke nagrade. Jedna važna pretpostavka jeste da agent ne dobija informaciju o tome da li je preduzeta %akcija bila dobra ili ne. Okruženje se predstavlja pomoću Markovljevog procesa odlučivanja. U nastavku je dat %teorijski opis Markovljevih procesa odlučivanja.

\subsection{Osnovni pojmovi}
Pod pretpostavkom da se interakcija između agenta i okruženja izvršava u diskretnim trenucima, stanje okruženja u trenutku $t$ označava se sa $S_t$ dok se skup svih stanja označava sa $\mathcal{S}$.  Agent u stanju $S_t$ preduzima akciju $A_t \in \mathcal{A}(S_t)$ i prelazi u novo stanje, $S_{t+1}$ dobijajući nagradu $R_{t+1}$. $\mathcal{A}(s)$ označava skup dozvoljenih akcija u stanju $s$. Ukoliko se sa $\mathbb{A}$ označi skup svih akcija, $\mathcal{A}$ se može posmatrati kao funkcija:  $\mathcal{A}: \mathcal{S} \rightarrow  \mathbb{P}(\mathbb{A})$. Skup $\mathcal{R}$ je skup mogućih realnih nagrada. Sekvenca $S_0, A_0, R_1, S_1, A_1, R_2, S_2, ...$ naziva se putanjom i dobija se [naizmeničnom?] interakcijom agenta sa okruženjem. Putanja može biti i konačna i beskonačna.
Neophodno je definisati i funkciju prelaska, $p$:
\begin{equation}
	p(s', r~|~s, a) \eqdef P(S_{t+1} = s', R_{t+1} = r ~|~ S_t = s, A_t = a)
\end{equation}
koja predstavlja verovatnoću prelaska u stanje $s'$ i dobijanje nagrade $r$ pod uslovom da je u stanju $s$ preduzeta akcija $a$. Markovljevi procesi odlučivanja imaju takozvano Markovljevo svojstvo tj. osobinu da trenutno stanje i nagrada zavise isključivo od prethodnog stanja i u njemu preduzete akcije a ne od cele putanje koja je dovela do datog stanja. Formalno, ovo svojstvo zapisuje se sledećom jednakošću:
\begin{equation}
	P(S_t, R_t ~|~ S_0, A_0, R_1, ..., S_{t-1}, A_{t-1}) = P(S_t, R_t ~|~ S_{t-1}, A_{t-1})
\end{equation}

U odnosu na putanju od trenutka $t$, može se govoriti o dugoročnoj nagradi, koja se još naziva i dobitkom:
\begin{equation}
\label{eq:dug_suma}
	G_t = \sum_{i=0}^{\infty} \gamma^iR_{t+i+1}
\end{equation}
Metaparametar $\gamma$ naziva se umanjenjem i ukazuje na to koliko se značaja pridaje kasnije dobijenim nagradama u odnosu na neposrednu nagradu. Za $\gamma = 0$, buduće nagrade nisu bitne dok postavljanje vrednosti $\gamma$ na $1$ ukazuje na to da se smatra da su sve nagrade putanje jednako bitne. Iz ove sume uočava se i odnos sa dugoročnom nagradom od trenutka $t+1$:
\begin{equation}
	\begin{aligned}
		G_t &= \sum_{i=0}^{\infty} \gamma^iR_{t+i+1} \\
        	&= R_{t+1} + \gamma\sum_{i=0}^{\infty} \gamma^iR_{(t+1)+i+1} \\
        	&=R_{t+1} + \gamma G_{t+1}
	\end{aligned}
\end{equation}
\par 
Sada je moguće dati formalnu definiciju: Markovljev proces odlučivanja je uređena petorka $(\mathcal{S}, \mathcal{A}, \mathcal{R}, p, \gamma)$. Ova definicija je prilično jednostavna a ipak je dovoljno fleksibilna za formalne opise raznih modela. Ako su skupovi $\mathcal{S}$, $\mathcal{R}$, $\mathcal{A}(s)$, za svako $s$, konačni, tada se za Markovljev proces odlučivanja kaže da je konačan. Umesto zahteva da $\mathcal{A}(s)$ bude konačan za svako $s$, može se zahtevati da je skup $\mathbb{A}$ konačan.
\par 
Markovljevi procesi odlučivanja koriste se za modeliranje interakcije sa okruženjem i donošenje odluka. Ova primena pokazala se izuzetno pogodno za probleme učenja potrepljivanjem. Cilj agenta biće maksimizacija dugoročne nagrade prilikom interakcije sa okruženjem.


\subsubsection{Epizode}

U jednakosti \eqref{eq:dug_suma} pretpostavlja se da niz interakcija sa okruženjem, tj. putanja, traje beskonačno, što se vidi iz gornje granice u sumi. Međutim, često je prirodnije pretpostaviti da su putanje konačne i da se završavaju u nekom posebnom stanju iz kog nije moguće dalje preduzimati akcije. Ovakva stanja nazivaju se završnim stanjima. Takvih stanja može biti više ali, zbog načina na koji je definisana funkcija prelaska\footnote{Agent dobija numeričku nagradu za preduzimanje akcije u stanju a ne za dolazak u stanje}, za time nema potrebe i bez gubitka opštosti se može pretpostaviti da je ovakvo stanje, ukoliko postoji, jedinstveno. Jedan niz interakcija agenta sa okruženjem koji se završava završnim stanjem naziva se epizodom. Epizode su međusobno nezavisne u smislu da ishod jedne epizode ni na koji način ne utiče na neku od narednih epizoda, što se tiče samog okruženja. Ukoliko agent treba da igra, na primer, šah, partije se mogu smatrati epizodama. 
\par 
Neki problemi ne mogu se razbiti na epizode. Ovi problemi predstavljaju dugoročne zadatke kao što su beskonačno balansiranje uspravnog štapa ili odbrana od nadolazećih talasa neprijatelja u slučaju nekih video igara. Kod ovakvih problema, jako je važno postaviti umanjenje na vrednost manju od $1$. Naime, ukoliko važi $\gamma < 1$ i niz nagrada $R_t$ je ograničen, tada će suma \eqref{eq:dug_suma} konvergirati. Ukoliko ta suma divergira, odnosno ako je njena vrednost $\infty$ ili neodređena, tada će maksimizacija postati trivijalna, odnosno nemoguća.
\par 
Iz stanovišta završnih stanja i dugoročne nagrade, ova dva slučaja mogu se objediniti bez izmene \eqref{eq:dug_suma}. Kod problema koji se ne mogu podeliti na epizode, suma ostaje ista uz zahtev da je umanjenje strogo manje od $1$. Kod problema koji se mogu podeliti na epizode, može se uvesti pretpostavka da će se iz završnog stanja sa verovatnoćom $1$ prelaziti u to isto stanje uz vrednost nagrade $0$. Ovo je neizvodljivo za implementaciju pa se u praksi koristi konačna suma oblika:
\begin{equation}
	G_t = \sum_{i=0}^{T} \gamma^iR_{t+i+1}
\end{equation}
gde je $t+T+1$ trenutak kraja epizode. 

\subsubsection{Politika; vrednosti stanja i akcije}

Kako je neophodno opisati pravila ponašanja agenta, uvodi se funkcija $\pi$, koja predstavlja verovatnoću da agent u stanju $s$ preduzme akciju $a$. Ova funkcija naziva se politikom i, ukoliko neki agent preduzima akciju $a$ u stanju $s$ sa verovatnoćom $\pi(a~|~s)$, za sva stanja $s$ i sve akcije $a$, kaže se da agent prati politiku $\pi$. 

Ukoliko za neku politiku $\pi$ važi da za svako $s$ postoji stanje $a_s$ takvo da $\pi(a_s~|~s)=1$ i $\pi(a~|~s)=0$ za sve ostale akcije $a$, tada se kaže da je politika deterministička. Radi jednostavnosti, u tom slučaju može se napisati $\pi(s)=a_s$.
\par 
Ako je poznata politika $\pi$, može se definisati funkcija vrednosti stanja pri praćenju politike $\pi$:
\begin{equation}
	v_{\pi}(s) \eqdef \mathbb{E}_{\pi}[G_t~|~S_t=s]
\end{equation}
Na sličan način uvodi se i funkcija vrednosti akcije u stanju pri praćenju politike $\pi$:
\begin{equation}
	q_{\pi}(s, a) \eqdef \mathbb{E}_{\pi}[G_t~|~S_t=s, A_t=a]
\end{equation}
Simbol $\mathbb{E}_{\pi}$ označava matematičko očekivanje ako se podrazumeva da se pri preduzimanju akcija prati politika $\pi$. Radi jednostavnostu će u nastavku funkcija $v_{\pi}$, odnosno $q_{\pi}$, biti nazivana samo funkcija vrednosti stanja, odnosno funkcija vrednosti akcije u stanju, dok će politika biti zapisana u indeksu.
\par 
Mnogi algoritmi učenja potkrepljivanjem zasnivaju se na nalaženju optimalne politike, odnosno politike čijim se praćenjem dolazi do maksimalne dugoročne nagrade. Moguće je uvesti parcijalno uređenje politika definisano na sledeći način:
\begin{equation}
	\label{eq:nije_losija}
	\pi_1 \leq \pi_2 \eqivdef \big(\forall s \in \mathcal{S}\big) \big( v_{\pi_1}(s) \leq v_{\pi_2}(s)\big)
\end{equation}
i tada se kaže da politika $\pi_2$ nije lošija  od politike $\pi_1$. Ukoliko postoji neka politika $\pi_*$ koja nije lošija ni od jedne politike za dati Markovljev proces odlučivanja, tada se ona naziva optimalnom politikom. 

\subsubsection{Belmanove jednakosti; optimalna politika}

Funkcije $v_{\pi}$ i $q_{\pi}$ zadovoljavaju rekurentne relacije koje se zovu Belmanovim jednakostima. U daljim izvođenjima za funkciju $v_{\pi}$ podrazumeva se da jednakosti važe za sva stanja $s$ odnosno za sva stanja $s$ i dozvoljene akcije u tim stanjima, $a$, za $q_{\pi}$. Važi:
\begin{equation}
	\begin{aligned}
		v_{\pi}(s) &\eqdef \mathbb{E}_{\pi}[G_t~|~S_t=s] \\ 
		&= \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1}~|~S_t=s] \\
		&= \mathbb{E}_{\pi}[R_{t+1}~|~S_t=s] + \gamma \mathbb{E}_{\pi}[G_{t+1}~|~S_t=s]
	\end{aligned}
\end{equation}
Sada je neophodno izračunati oba sabirka. Prvi sabirak označava očekivanu neposrednu nagradu polazeći iz stanja $s$ i prateći politiku $\pi$:
\begin{equation}
	\begin{aligned}
		\mathbb{E}_{\pi}[R_{t+1}~|~S_t=s] = \sum_{a}^{} \pi(a~|~s)\sum_{s', r}^{}rp(s', r~|~s, a)
	\end{aligned}
\end{equation}
Iz stanja $s$ preduzima se akcija $a$ sa verovatnoćom $\pi(a~|~s)$, dok se za preduzetu akciju $a$ u stanju $s$ sa verovatnoćom $p(s', r~|~s, a)$ prelazi u stanje $s'$ uz dobijanje nagrade $r$. U drugoj sumi, indeksi su $s'$ i $r$, što označava dvostruku sumu, jednu po $s'$ i jednu po $r$.
\par 
Drugi sabirak, bez množioca $\gamma$, proširuje se na sledeći način:
\begin{equation}
	\begin{aligned}
		\mathbb{E}_{\pi}[G_{t+1}~|~S_t=s] &= \sum_{a}^{} \pi(a~|~s)\sum_{s',r}^{}p(s', r~|~s, a)\mathbb{E}_{\pi}[G_{t+1}~|~S_{t+1}=s'] \\
		&= \sum_{a}^{} \pi(a~|~s)\sum_{s',r}^{}p(s', r~|~s, a)v_{\pi}(s')
	\end{aligned}
\end{equation}
Sumiranje se vrši i po $r$ jer nije nužno da stanju $s'$ odgovara jedinstvena nagrada $r$.
\par 
Spajanjem dve jednakosti dobija se:
\begin{equation}
	\label{eq:bel_v}
	\begin{aligned}
		v_{\pi}(s)  &= \mathbb{E}_{\pi}[R_{t+1}~|~S_t=s] + \gamma \mathbb{E}_{\pi}[G_{t+1}~|~S_t=s] \\
		&= \sum_{a}^{} \pi(a~|~s)\sum_{s', r}^{}rp(s', r~|~s, a) + \gamma \sum_{a}^{} \pi(a~|~s)\sum_{s',r}^{}p(s', r~|~s, a)v_{\pi}(s') \\
		&= \sum_{a}^{} \pi(a~|~s)\sum_{s', r}^{}p(s', r~|~s, a) \big( r+\gamma v_{\pi}(s') \big)
	\end{aligned}
\end{equation}
Analogno se izvodi rekurentna veza za funkciju vrednosti akcije u stanju, $q_{\pi}$:
\begin{equation}
	\label{eq:bel_q}
	\begin{aligned}
		q_{\pi}(s,a) = \sum_{a}^{} \pi(a~|~s)\sum_{s', r}^{}\big( r+\gamma\sum_{a'}^{}\pi(a'~|~s') q_{\pi}(s', a') \big)
	\end{aligned}
\end{equation}
Jednakosti \eqref{eq:bel_v} i \eqref{eq:bel_q} nazivaju se Belmanovim jednakostima i ključne su za mnoge algoritme učenja potkrepljivanjem. 
\par 
Iz definicije optimalne politike, $\pi_*$, slede definicije optimalne funkcije vrednosti stanja i optimalne funkcije vrednosti akcije u stanju koje odgovaraju optimalnoj politici, u oznaci $v_*$ i $q_*$:
\begin{equation}
	\begin{aligned}
		v_*(s) &\eqdef \max_{\pi} v_{\pi} (s)	 \\
		q_*(s,a) &\eqdef \max_{\pi} q_{\pi}(s,a)
	\end{aligned}
\end{equation}
Ukoliko neki agent prati optimalnu politiku, ona će ga dovesti do maksimalne dugoročne nagrade. Ovo sledi iz činjenica da $v_{\pi}(s)$ predstavlja dugoročnu nagradu polazeći iz stanja $s$, i da $v_*$ za svako stanje $s$ predstavlja najveću vrednost stanja među svim politikama.
\par
Moguće je uspostaviti vezu između $v_*$ i $q_*$:
\begin{equation}
	\begin{aligned}
		v_*(s) &= \max_{a \in \mathcal{A}(s)} q_* (s,a)	 \\
		q_*(s,a) &= \mathbb{E}_{\pi}[R_{t+1} + \gamma v_*(s_{t+1})~|~S_t=s, A_t=a]
	\end{aligned}
\end{equation}
Relativno jednostavnim izvođenjem dolazi se do još jednog para jednakosti koje se nazivaju Belmanovim jednakostima optimalnosti.
	\begin{align}
		\label{eq:bel_v_opt} v_*(s) &= \max_{a}\sum_{s', r}^{} p(s', r~|~s,a)[r+\gamma v_*(s')]	 \\
		\label{eq:bel_q_opt} q_*(s,a) &= \sum_{s', r}^{} p(s', r~|~s,a)[r+\gamma \max_{a'}q_*(s',a')]
	\end{align}
\par 
U slučaju konačnih Markovljevih procesa odlučivanja, rekurentne relacije \eqref{eq:bel_v_opt} čine sistem od $n$ jednačina sa $n$ nepoznatih.\footnote{Kao što je pomenuto na početku dela o Belmanovim jednakostima, one važe za svako stanje $s$.} Isto važi i za relacije \eqref{eq:bel_q_opt}. Ovi sistemi sadrže funkciju $max$ pa stoga nisu linearni. Ukoliko je funkcija prelaska za dati MDP poznata, ovi sistemi se mogu rešiti; ta rešenja dobijena su bez prethodnog znanja o optimalnoj politici. 
\par 
Ukoliko je $v_*$ poznata, moguće je odrediti optimalnu politiku. Iz jednakosti \eqref{eq:bel_v_opt} jasno je da postoji jedna ili više akcija koje u stanju $s$ dovode do maksimalne sume. Bilo koja politika koja nekim od ovih akcija dodeljuje nenula vrednosti a svim ostalim dodeljuje vrednost $0$ je optimalna. Ukoliko je poznata funkcija $q_*$, nalaženje optimalne politike još je jednostavnije: u svakom stanju $s$ preduzima se akcija $a$ takva da se maksimizuje $q_*(s,a)$. Poznavanje $q_*$, dakle, omogućuje nalaženje optimalne politike bez ikakvog uvida u funkciju prelaska.  Međutim, ovaj direktni pristup nalaženju $v_*$ ili $q_*$ obično se ne koristi jer funkcija prelaska u praksi uglavnom nije poznata.

\section{Rešavanje Markovljevih procesa odlučivanja}

Uz pretpostavku da se radi o konačnom MDP, osnovni pristup njegovom rešavanju jeste koristeći principe dinamičkog programiranja. Kako je cilj učenja traženje optimalne politike, dve politike biće poređene u skladu sa definicijom \eqref{eq:nije_losija}, odnosno koristeći funkciju vrednosti politike. Dakle, neophodno je naći metod kojim se računa funkcija vrednosti stanja koja odgovara nekoj politici $\pi$.  Način na koji se ovo radi je korišćenjem Belmanovih jednakosti, \eqref{eq:bel_v}. Naime, počinje se od vektora $v_0$ koji sadrži proizvoljnu vrednost i dužine je koja odgovara broju stanja. Dalje, počev od vektora $v_0$, primenjuje se iterativno ažuriranje po sledećem pravilu:
\begin{equation}
		v_{k+1}(s) = \sum_{a}^{} \pi(a~|~s)\sum_{s', r}^{}p(s', r~|~s, a) \big( r+\gamma v_k(s') \big)
\end{equation}
Pod uslovom da $\gamma < 1$ ili da je svaka putanja iz svakog stanja konačna, niz vektora $v_k$ konvergira ka $v_{\pi}$. Sada je i u praksi moguće uporediti dve politike.
\par 
Kada je data deterministička politika $\pi$, postavlja se pitanje da li je moguće unaprediti je, tj. da li u stanju $s$ odabrati akciju $\pi(s)$ ili neku drugu akciju? Tada je neophodno ispitati vrednost izraza $q_{\pi}(s,a)$ u odnosu na izraz $v_{\pi}(s)$. Ako postoji akcija $a$ takva da je $q_{\pi}(s, a) \geq v_{\pi}(s)$, tada je moguće konstruisati politiku $\pi'$ takvu da važi $\pi' \geq \pi$. Ovo važi na osnovu teoreme o unapređenju politike (eng.~{\em policy improvement theorem}):
\begin{theorem*}
	Neka su $\pi$ i $\pi'$ dve determinističke politike takve da za sve $s \in \mathcal{S}$ važi $q_{\pi}(s, \pi'(s)) \geq v_{\pi}(s)$. Tada politika $\pi'$ nije lošija od politike $\pi$, odnosno $\pi' \geq \pi$.
\end{theorem*}
Dokaz teoreme relativno je jednostavan i može se naći u [Kako da ubacim tacnu stranu gde se nalazi teorema? Da li da ubacim jos jednu stavku u bibliografiju?].
Preostalo je da se za proizvoljnu politiku $\pi$ nađe politika $\pi'$ takva da $\pi' \geq \pi$. Ukoliko se u svakom stanju $s$ odabere akcija $a$ koja maksimizuje izraz $q_{\pi}(s, a)$, tada novodobijena politika nije lošija od stare zbog gore navedene teoreme. Dakle, od politike $\pi$ dobija se politika $\pi'$ na sledeći način:
\begin{equation}
	\pi'(s) = \argmax_a q_{\pi}(s, a)
\end{equation}
Proces dobijanja $\pi'$ od $\pi$  naziva se unapređenjem politike (eng.~{\em policy improvement}) i omogućuje iterativni pristup traženju optimalne politike. 

