\chapter{Učenje potkrepljivanjem}
\label{ch:rl}

Učenje potkrepljivanjem je vid mašinskog učenja koji podseća na učenje koje se može naći u prirodi: ljudi i životinje uče na osnovu interakcije sa svetom oko sebe. Razlikovanje povoljnog i nepovoljnog ponašanja nije unapred učinjeno već je neophodno da onaj koji uči donese taj zaključak. U učenju potkrepljivanjem kao grani mašinskog učenja, komunikacija sa okruženjem svodi se na preduzimanje akcija u nekoj situaciji i dobijanja odgovora u vidu numeričke nagrade i informacije o tome kako se situacija promenila. Entitet koji komunicira sa svetom naziva se (softverskim) agentom dok se svet naziva okruženjem. Kao i kod ostalih oblasti mašinskog učenja, učenje potkrepljivanjem podrazumeva skup problema i njihovih rešenja. Cilj je baš učenje na osnovu komunikacije sa okruženjem, bez potrebe za ljudskom intervencijom.

\section{Osnovni pojmovi}

Učenje potkrepljivanjem počiva na četiri komponente: politici ponašanja (ili samo politici), nagradi, funkciji vrednosti i modelu okruženja. Politika opisuje način na koji se agent ponaša. Nagrada predstavlja numerički signal koji agent dobija od okruženja. U toku učenja, cilj agenta je da maksimizuje ukupnu nagradu dobijenu od okruženja. Dakle, nagradom je implicitno objašnjeno šta je dobro a šta loše ponašanje. Prilikom rešavanja problema učenja potkrepljivanjem, cilj je nalaženje optimalne politike, tj. politike čijim se praćenjem dobija najveća dugoročna nagrada. Funkcija vrednosti govori koliko je dobro naći se u nekom stanju okruženja. Za razliku od nagrade, funkcija vrednosti opisuje kvalitet nekog stanja na duže staze. Ova komponenta je neophodna jer je moguće da dolaskom u neko stanje agent dobije malu nagradu ali da dato stanje ima veliku vrednost, što znači da je dolaskom u to stanje moguće ostvariti veliku dugoročnu nagradu. Neki algoritmi učenja potkrepljivanjem koriste model okruženja kako bi planirali unapred. Moguće je koristiti pristupe koji koriste model, pristupe koji ne koriste model već se uči iz iskustva, kao i pristupe koji koriste učenje iz iskustva radi učenja modela.
\par 
U učenju potkrepljivanjem javlja se potreba za uspostavljanjem balansa između istraživanja i iskorišćavanja već stečenog znanja (eng.~{\em exloration vs. exploitation}). Naime, na početku učenja, agent istražuje okruženje i time uči kako bi trebalo da se ponaša. Čak i kada se nauči neko ponašanje, često je neophodno nastaviti sa istraživanjem u nekoj meri. Kako već naučeno ponašanje možda nije najbolje, bez istraživanja je moguće završiti u nekom od lokalnih optimuma. Međutim, kako učenje teče i agent unapređuje svoje ponašanje, poželjno je pratiti politiku koja dovodi do velikih dugoročnih nagrada. Najčešće je stopa istraživanja velika na početku učenja i opada u toku ovog procesa.

\section{Markovljevi procesi odlučivanja}
\label{sec:mdp}

Markovljevi procesi odlučivanja (eng. {\em Markov decision process}, skr. {\em MDP}) daju teorijski okvir u kome je moguće relativno jednostavno postaviti i rešiti problem učenja potkrepljivanjem. Markovljevi procesi odlučivanja opisuju okruženje s kojim je moguće komunicirati. Ta komunikacija sastoji se iz toga da se od okruženja može dobiti informacija o stanju i da se okruženju može poslati informacija o akciji koja se preduzima, na šta okruženje odgovara informacijama o novom stanju i nagradi. Izuzetno je važno da se od okruženja ne može dobiti informacija o tome da li je preduzeta akcija prava ili ne već samo informacija o nagradi, koju na duže staze treba maksimizovati. Kako se pri učenju potkrepljivanjem pretpostavlja postojanje agenta, kao i postojanje okruženja, nadalje se podrazumeva da postoji neki agent koji komunicira sa okruženjem. Treba imati u vidu da Markovljevi procesi odlučivanja opisuju idealno okruženje. U praksi okruženja često nisu idealna i tada se pribegava metodima koji ne koriste MDP direktno. Iako je veliki deo teorije u učenju potkrepljivanjem ograničen pretpostavkom korišćenja MDP, iste ideje imaju širu primenu.

\subsection{Osnovni pojmovi}
Pod pretpostavkom da se interakcija između agenta i okruženja izvršava u diskretnim trenucima, stanje okruženja u trenutku $t$ označava se sa $S_t$ dok se skup svih stanja označava sa $\mathcal{S}$.  Agent u stanju $S_t$ preduzima akciju $A_t \in \mathcal{A}(S_t)$ i prelazi u novo stanje, $S_{t+1}$ dobijajući nagradu $R_{t+1}$. $\mathcal{A}(s)$ označava skup dozvoljenih akcija u stanju $s$. Ukoliko se sa $\mathbb{A}$ označi skup svih akcija, $\mathcal{A}$ se može posmatrati kao funkcija  $\mathcal{A}: \mathcal{S} \rightarrow  \mathbb{P}(\mathbb{A})$.\footnote{$\mathbb{P}(\mathbb{A})$ označava partitivni skup skupa $\mathbb{A}$.} Skup $\mathcal{R}$ je skup mogućih realnih nagrada. Sekvenca $S_0, A_0, R_1, S_1, A_1, R_2, S_2, ...$ naziva se putanjom i dobija se interakcijom agenta sa okruženjem. Putanja može biti ili konačna i beskonačna.
Neophodno je definisati i funkciju prelaska, $p$:
\begin{equation}
	p(s', r~|~s, a) \eqdef P(S_{t+1} = s', R_{t+1} = r ~|~ S_t = s, A_t = a)
\end{equation}
koja predstavlja verovatnoću prelaska u stanje $s'$ i dobijanje nagrade $r$ pod uslovom da je u stanju $s$ preduzeta akcija $a$. Još jedna bitna pretpostavka je da je $p$ raspodela verovatnoće, iz čega sledi da:
\begin{equation}
	\sum_{s', r}^{} p(s', r~|~s, a) = 1
\end{equation}
za sve $s$ i $a \in \mathcal{A}(s)$. Ova skraćena oznaka za dvostruku sumu biće korišćena nadalje i označava sumiranje po svim $r \in \mathcal{R}$ i svim $s' \in \mathcal{S}$.
\par 
Markovljevi procesi odlučivanja imaju takozvano Markovljevo svojstvo tj. osobinu da trenutno stanje i nagrada zavise isključivo od prethodnog stanja i u njemu preduzete akcije a ne od cele putanje koja je dovela do datog stanja. Formalno, ovo svojstvo zapisuje se sledećom jednakošću:
\begin{equation}
	P(S_t, R_t ~|~ S_0, A_0, R_1, ..., S_{t-1}, A_{t-1}) = P(S_t, R_t ~|~ S_{t-1}, A_{t-1})
\end{equation}

U odnosu na putanju od trenutka $t$, može se govoriti o dugoročnoj nagradi, koja se još naziva i dobitkom:
\begin{equation}
\label{eq:dug_suma}
	G_t = \sum_{i=0}^{\infty} \gamma^iR_{t+i+1}
\end{equation}
Metaparametar $\gamma$ naziva se umanjenjem i ukazuje na to koliko se značaja pridaje kasnije dobijenim nagradama u odnosu na neposrednu nagradu. Za $\gamma = 0$\footnote{$0^0$ definiše se kao $1$.}, buduće nagrade nisu bitne dok postavljanje vrednosti $\gamma$ na $1$ ukazuje na to da se smatra da su sve nagrade putanje podjednako bitne. Iz ove sume uočava se i odnos sa dugoročnom nagradom od trenutka $t+1$:
\begin{equation}
	\begin{aligned}
		G_t &= \sum_{i=0}^{\infty} \gamma^iR_{t+i+1} \\
        	&= R_{t+1} + \gamma\sum_{i=0}^{\infty} \gamma^iR_{(t+1)+i+1} \\
        	&=R_{t+1} + \gamma G_{t+1}
	\end{aligned}
\end{equation}
\par 
Sada je moguće dati formalnu definiciju: Markovljev proces odlučivanja je uređena petorka $(\mathcal{S}, \mathcal{A}, \mathcal{R}, p, \gamma)$. Ova definicija je prilično jednostavna a ipak je dovoljno fleksibilna za formalne opise raznih modela. Ako su skupovi $\mathcal{S}$, $\mathcal{R}$, $\mathcal{A}(s)$, za svako $s$, konačni, tada se za Markovljev proces odlučivanja kaže da je konačan. Umesto zahteva da $\mathcal{A}(s)$ bude konačan za svako $s$, može se zahtevati da je skup $\mathbb{A}$ konačan.
\par 
Markovljevi procesi odlučivanja koriste se za modeliranje interakcije sa okruženjem i donošenje odluka. Ova primena pokazala se izuzetno pogodno za probleme učenja potrepljivanjem. Cilj agenta biće maksimizacija dugoročne nagrade prilikom interakcije sa okruženjem.


\subsubsection{Epizode}

U jednakosti \eqref{eq:dug_suma} pretpostavlja se da niz interakcija sa okruženjem, tj. putanja, traje beskonačno, što se vidi iz gornje granice u sumi. Međutim, često je prirodnije pretpostaviti da su putanje konačne i da se završavaju u nekom posebnom stanju iz kog nije moguće dalje preduzimati akcije. Ovakva stanja nazivaju se završnim stanjima. Takvih stanja može biti više ali, zbog načina na koji je definisana funkcija prelaska\footnote{Agent dobija nagradu za preduzimanje akcije u stanju a ne za dolazak u stanje.}, za time nema potrebe i bez gubitka opštosti se može pretpostaviti da je ovakvo stanje, ukoliko postoji, jedinstveno. Jedan niz interakcija agenta sa okruženjem koji se završava završnim stanjem naziva se epizodom. Epizode su međusobno nezavisne u smislu da ishod jedne epizode ni na koji način ne utiče na neku od narednih epizoda, što se tiče samog okruženja. Ukoliko agent treba da igra, na primer, šah, partije se mogu smatrati epizodama. 
\par 
Neki problemi ne mogu se razbiti na epizode. Ovi problemi predstavljaju dugoročne zadatke kao što su beskonačno balansiranje uspravnog štapa ili odbrana od nadolazećih talasa neprijatelja u slučaju nekih video igara. Kod ovakvih problema, jako je važno postaviti umanjenje na vrednost manju od $1$. Naime, ukoliko važi $\gamma < 1$ i niz nagrada $R_t$ je ograničen, tada će suma \eqref{eq:dug_suma} konvergirati. Ukoliko ta suma divergira, odnosno ako je njena vrednost $\infty$ ili neodređena, tada će maksimizacija postati trivijalna, odnosno nemoguća.
\par 
Sa stanovišta završnih stanja i dugoročne nagrade, ova dva slučaja mogu se objediniti bez izmene \eqref{eq:dug_suma}. Kod problema koji se ne mogu podeliti na epizode, suma ostaje ista uz zahtev da je umanjenje strogo manje od $1$. Kod problema koji se mogu podeliti na epizode, može se uvesti pretpostavka da će se iz završnog stanja sa verovatnoćom $1$ prelaziti u to isto stanje uz vrednost nagrade $0$. Ovo je neizvodljivo za implementaciju pa se u praksi koristi konačna suma oblika:
\begin{equation}
	G_t = \sum_{i=0}^{T} \gamma^iR_{t+i+1}
\end{equation}
gde je $t+T+1$ trenutak kraja epizode. 

\subsubsection{Politika; vrednosti stanja i akcije}

Kako je neophodno opisati pravila ponašanja agenta, uvodi se funkcija $\pi$, koja predstavlja verovatnoću da agent u stanju $s$ preduzme akciju $a$. Ova funkcija naziva se politikom i, ukoliko neki agent preduzima akciju $a$ u stanju $s$ sa verovatnoćom $\pi(a~|~s)$, za sva stanja $s$ i sve akcije $a$, kaže se da agent prati politiku $\pi$. Vrednosti $\pi(a~|~s)$, kad $a \notin \mathcal{A}(s)$, se ne razmatraju.

Ukoliko za neku politiku $\pi$ važi da za svako $s$ postoji stanje $a_s$ takvo da $\pi(a_s~|~s)=1$ i $\pi(a~|~s)=0$ za sve ostale akcije $a$, tada se kaže da je politika deterministička. Radi jednostavnosti, u tom slučaju se može napisati $\pi(s)=a_s$.
\par 
Ako je poznata politika $\pi$, može se definisati funkcija vrednosti stanja pri praćenju politike $\pi$:
\begin{equation}
	v_{\pi}(s) \eqdef \mathbb{E}_{\pi}[G_t~|~S_t=s]
\end{equation}
Na sličan način uvodi se i funkcija vrednosti akcije u stanju pri praćenju politike $\pi$:
\begin{equation}
	q_{\pi}(s, a) \eqdef \mathbb{E}_{\pi}[G_t~|~S_t=s, A_t=a]
\end{equation}
Simbol $\mathbb{E}_{\pi}$ označava matematičko očekivanje ako se podrazumeva da se pri preduzimanju akcija prati politika $\pi$. Radi jednostavnostu će u nastavku funkcija $v_{\pi}$, odnosno $q_{\pi}$, biti nazivana samo funkcija vrednosti stanja, odnosno funkcija vrednosti akcije u stanju, dok će oznaka politike biti zapisana u indeksu.
\par 
Mnogi algoritmi učenja potkrepljivanjem zasnivaju se na nalaženju optimalne politike, odnosno politike čijim se praćenjem dolazi do maksimalne dugoročne nagrade. Moguće je uvesti parcijalno uređenje politika definisano na sledeći način:
\begin{equation}
	\label{eq:nije_losija}
	\pi_1 \leq \pi_2 \eqivdef \big(\forall s \in \mathcal{S}\big) \big( v_{\pi_1}(s) \leq v_{\pi_2}(s)\big)
\end{equation}
i tada se kaže da politika $\pi_2$ nije lošija  od politike $\pi_1$. Ukoliko postoji neka politika $\pi_*$ koja nije lošija ni od jedne politike za dati Markovljev proces odlučivanja, tada se ona naziva optimalnom politikom. 

\subsubsection{Belmanove jednakosti; optimalna politika}

Funkcije $v_{\pi}$ i $q_{\pi}$ zadovoljavaju rekurentne relacije koje se zovu Belmanovim jednakostima. U daljim izvođenjima za funkciju $v_{\pi}$ podrazumeva se da jednakosti važe za sva stanja $s$ odnosno za sva stanja $s$ i dozvoljene akcije u tim stanjima, $a$, za $q_{\pi}$. Važi:
\begin{equation}
	\begin{aligned}
		v_{\pi}(s) &\eqdef \mathbb{E}_{\pi}[G_t~|~S_t=s] \\ 
		&= \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1}~|~S_t=s] \\
		&= \mathbb{E}_{\pi}[R_{t+1}~|~S_t=s] + \gamma \mathbb{E}_{\pi}[G_{t+1}~|~S_t=s]
	\end{aligned}
\end{equation}
Sada je neophodno izračunati oba sabirka. Prvi sabirak označava očekivanu neposrednu nagradu polazeći iz stanja $s$ i prateći politiku $\pi$:
\begin{equation}
	\begin{aligned}
		\mathbb{E}_{\pi}[R_{t+1}~|~S_t=s] = \sum_{a}^{} \pi(a~|~s)\sum_{s', r}^{}rp(s', r~|~s, a)
	\end{aligned}
\end{equation}
Iz stanja $s$ preduzima se akcija $a$ sa verovatnoćom $\pi(a~|~s)$, dok se za preduzetu akciju $a$ u stanju $s$ sa verovatnoćom $p(s', r~|~s, a)$ prelazi u stanje $s'$ uz dobijanje nagrade $r$. 
\par 
Drugi sabirak, bez množioca $\gamma$, proširuje se na sledeći način:
\begin{equation}
	\begin{aligned}
		\mathbb{E}_{\pi}[G_{t+1}~|~S_t=s] &= \sum_{a}^{} \pi(a~|~s)\sum_{s',r}^{}p(s', r~|~s, a)\mathbb{E}_{\pi}[G_{t+1}~|~S_{t+1}=s'] \\
		&= \sum_{a}^{} \pi(a~|~s)\sum_{s',r}^{}p(s', r~|~s, a)v_{\pi}(s')
	\end{aligned}
\end{equation}
Sumiranje se vrši i po $r$ jer nije nužno da stanju $s'$ odgovara jedinstvena nagrada $r$.
\par 
Spajanjem dve jednakosti dobija se:
\begin{equation}
	\label{eq:bel_v}
	\begin{aligned}
		v_{\pi}(s)  &= \mathbb{E}_{\pi}[R_{t+1}~|~S_t=s] + \gamma \mathbb{E}_{\pi}[G_{t+1}~|~S_t=s] \\
		&= \sum_{a}^{} \pi(a~|~s)\sum_{s', r}^{}rp(s', r~|~s, a) + \gamma \sum_{a}^{} \pi(a~|~s)\sum_{s',r}^{}p(s', r~|~s, a)v_{\pi}(s') \\
		&= \sum_{a}^{} \pi(a~|~s)\sum_{s', r}^{}p(s', r~|~s, a) \big[ r+\gamma v_{\pi}(s') \big]
	\end{aligned}
\end{equation}
Analogno se izvodi rekurentna veza za funkciju vrednosti akcije u stanju, $q_{\pi}$:
\begin{equation}
	\label{eq:bel_q}
	\begin{aligned}
		q_{\pi}(s,a) = \sum_{a}^{} \pi(a~|~s)\sum_{s', r}^{}\big[ r+\gamma\sum_{a'}^{}\pi(a'~|~s') q_{\pi}(s', a') \big]
	\end{aligned}
\end{equation}
Jednakosti \eqref{eq:bel_v} i \eqref{eq:bel_q} nazivaju se Belmanovim jednakostima i ključne su za mnoge algoritme učenja potkrepljivanjem. 
\par 
Iz definicije optimalne politike, $\pi_*$, slede definicije optimalne funkcije vrednosti stanja i optimalne funkcije vrednosti akcije u stanju koje odgovaraju optimalnoj politici, u oznaci $v_*$ i $q_*$:
\begin{equation}
	\begin{aligned}
		v_*(s) &\eqdef \max_{\pi} v_{\pi} (s)	 \\
		q_*(s,a) &\eqdef \max_{\pi} q_{\pi}(s,a)
	\end{aligned}
\end{equation}
Ukoliko neki agent prati optimalnu politiku, ona će ga dovesti do maksimalne dugoročne nagrade. Ovo sledi iz činjenica da $v_{\pi}(s)$ predstavlja dugoročnu nagradu polazeći iz stanja $s$, i da $v_*$ za svako stanje $s$ predstavlja najveću vrednost stanja među svim politikama.
\par
Moguće je uspostaviti vezu između $v_*$ i $q_*$:
\begin{equation}
	\begin{aligned}
		v_*(s) &= \max_{a \in \mathcal{A}(s)} q_* (s,a)	 \\
		q_*(s,a) &= \mathbb{E}_{\pi}\big[R_{t+1} + \gamma v_*(s_{t+1})~|~S_t=s, A_t=a\big]
	\end{aligned}
\end{equation}
Relativno jednostavnim izvođenjem dolazi se do još jednog para jednakosti koje se nazivaju Belmanovim jednakostima optimalnosti.
	\begin{align}
		\label{eq:bel_v_opt} v_*(s) &= \max_{a}\sum_{s', r}^{} p(s', r~|~s,a)\big[r+\gamma v_*(s')\big]	 \\
		\label{eq:bel_q_opt} q_*(s,a) &= \sum_{s', r}^{} p(s', r~|~s,a)\big[r + \gamma \max_{a'}q_*(s',a')\big]
	\end{align}
\par 
U slučaju konačnih Markovljevih procesa odlučivanja, rekurentne relacije \eqref{eq:bel_v_opt} čine sistem od $n=|\mathcal{S}|$ jednačina sa $n$ nepoznatih.\footnote{Kao što je pomenuto na početku dela o Belmanovim jednakostima, one važe za svako stanje $s$.} Isto važi i za relacije \eqref{eq:bel_q_opt}. Ovi sistemi sadrže funkciju $max$ pa stoga nisu linearni. Ukoliko je funkcija prelaska za dati MDP poznata, ovi sistemi se mogu rešiti; ta rešenja dobijena su bez prethodnog znanja o optimalnoj politici. 
\par 
Ukoliko je $v_*$ poznata, moguće je odrediti optimalnu politiku. Iz jednakosti \eqref{eq:bel_v_opt} jasno je da postoji jedna ili više akcija koje u stanju $s$ dovode do maksimalne sume. Bilo koja politika koja nekim od ovih akcija dodeljuje nenula vrednosti a svim ostalim dodeljuje vrednost $0$ je optimalna. Ukoliko je poznata funkcija $q_*$, nalaženje optimalne politike još je jednostavnije: u svakom stanju $s$ preduzima se akcija $a$ takva da se maksimizuje $q_*(s,a)$. Poznavanje $q_*$, dakle, omogućuje nalaženje optimalne politike bez ikakvog uvida u funkciju prelaska.  Međutim, ovaj direktni pristup nalaženju $v_*$ ili $q_*$ obično se ne koristi jer funkcija prelaska u praksi uglavnom nije poznata.

\section{Rešavanje Markovljevih procesa odlučivanja}

Uz pretpostavku da se radi o konačnom MDP, osnovni pristup njegovom rešavanju jeste koristeći principe dinamičkog programiranja. Kako je cilj učenja traženje optimalne politike, dve politike biće poređene u skladu sa definicijom \eqref{eq:nije_losija}, odnosno koristeći funkciju vrednosti politike. Stoga, prvo je neophodno naći metod kojim se računa funkcija vrednosti stanja koja odgovara nekoj politici $\pi$.  Način na koji se ovo radi je korišćenjem Belmanovih jednakosti, \eqref{eq:bel_v}. Počinje se od vektora $v_0$ koji sadrži proizvoljne vrednosti i dužine je koja odgovara broju stanja. Dalje se primenjuje iterativno ažuriranje po sledećem pravilu:
\begin{equation}
		v_{k+1}(s) = \sum_{a}^{} \pi(a~|~s)\sum_{s', r}^{}p(s', r~|~s, a) \big[ r+\gamma v_k(s') \big]
\end{equation}
za sve $s \in \mathcal{S}$. Pod uslovom da $\gamma < 1$, niz vektora $v_k$ konvergira ka $v_{\pi}$. Proces se zaustavlja kada razlika dve uzastopne funkcije, $v_k$ i $v_{k+1}$, postane dovoljno mala, u skladu sa normom $\norm{\cdot}_{\infty}$ definisanom na sledeći način:
\begin{equation}
	\label{eq:norm_conv}
	\norm{v}_{\infty} = \max_x|v(x)|
\end{equation}
Sada je i u praksi moguće uporediti dve politike.
\par 
Kada je data deterministička politika $\pi$, postavlja se pitanje da li je moguće unaprediti je, tj. da li u stanju $s$ odabrati akciju $\pi(s)$ ili neku drugu akciju? Tada je za sve akcije $a$ dozvoljene u $s$ neophodno ispitati vrednost $q_{\pi}(s,a)$ u odnosu na vrednost $v_{\pi}(s)$. Ako neko $s$ postoji akcija $a$ takva da je $q_{\pi}(s, a) \geq v_{\pi}(s)$, tada je moguće konstruisati politiku $\pi'$ takvu da važi $\pi' \geq \pi$. Ovo važi na osnovu teoreme o unapređenju politike (eng.~{\em policy improvement theorem}):
\begin{theorem*}
	Neka su $\pi$ i $\pi'$ dve determinističke politike takve da za sve $s \in \mathcal{S}$ važi $q_{\pi}(s, \pi'(s)) \geq v_{\pi}(s)$. Tada politika $\pi'$ nije lošija od politike $\pi$, odnosno $\pi' \geq \pi$.
\end{theorem*}
Dokaz teoreme relativno je jednostavan i može se naći u \cite{rli_tup}. Preostalo je da se za proizvoljnu politiku $\pi$ nađe politika $\pi'$ takva da za svako $s$ važi $q_{\pi}(s, \pi'(s)) \geq v_{\pi}(s)$. Ukoliko se u svakom stanju $s$ odabere akcija $a$ koja maksimizuje izraz $q_{\pi}(s, a)$, tada tako dobijena politika zadovoljava uslove teoreme. Dakle, od politike $\pi$ dobija se politika $\pi'$ tako što se za svako $s \in \mathcal{S}$ računa:
\begin{equation}
	\pi'(s) = \argmax_a q_{\pi}(s, a)
\end{equation}
Proces dobijanja $\pi'$ od $\pi$  naziva se unapređenjem politike (eng.~{\em policy improvement}) i omogućuje iterativni pristup traženju optimalne politike. Politika $\pi'$ naziva se pohlepnom politikom jer se njenim praćenjem uvek bira najbolja akcija u neposrednom smislu. Ova politika zadovoljava uslove teoreme jer
\begin{align}
	\max_a q_{\pi}(s,a) \geq&~ q_{\pi}(s,\pi(s)) \\=&~v_{\pi}(s)
\end{align}
Druga jednakost sledi iz definicije funkcija $q_{\pi}$ i $v_{\pi}$.
\par 
Polazeći od nasumično kreirane determinističke politike i prateći ovaj postupak, naizmenično sa evaluacijom novodobijene politike, dobija se niz politika $\pi_0, \pi_1, ...$ koji u slučaju konačnih MDP konvergira optimalnoj politici, $\pi_*$. Ime ovog procesa je iterativno unapređenje politike (eng.~{\em policy iteration}). Nasumična deterministička politika $\pi$ kreira se tako što se za svaku vrednost $\pi(s)$ proizvoljno bira element iz $\mathcal{A}(s)$. Proces se zaustavlja kad $\pi'(s) = \pi(s)$, za sve $s \in \mathcal{S}$.
\par 
Iterativno unapređenje politike, iako relativno brzo konvergira u odnosu na broj stanja, zahteva evaluaciju politike u svakoj iteraciji. Postoji i algoritam iterativnog unapređenja funkcije vrednosti direktno, bez potrebe za naizmeničnim napređenjem i evaluacijom politike. Naime, umesto traženja optimalne politike, iterativno se traži optimalna funkcija vrednosti stanja, $v_*$. Na osnovu Belmanove jednakosti optimalnosti za funkciju vrednosti, \eqref{eq:bel_v_opt}, kreira se iterativno pravilo ažuriranja:
\begin{equation}
	\label{eq:val_iter} v_{k+1}(s) = \max_{a}\sum_{s', r}^{} p(s', r~|~s,a)[r+\gamma v_k(s')]
\end{equation}
gde se, kao i kod evaluacije politike, polazi od vektora $v_0$ čije su vrednosti nasumično odabrane. I ovaj proces konvergira u odnosu na normu ~$\norm{\cdot}_{\infty}$.
\par 
Pravilo ažuriranja \eqref{eq:val_iter} može se predstaviti kao operator $B:R^{|S|} \rightarrow R^{|S|}$ koji se naziva Belmanovim operatorom:
\begin{equation}
	Bv(s) = \max_{a}\sum_{s', r}^{} p(s', r~|~s,a)\big[r + \gamma v(s')\big]
\end{equation}
Ukoliko se pokaže da je operator $B$ kontrakcija, tada po teoremi o fiksnoj tački važi da postoji vektor $v_*$ takav da $Bv_*=v_*$. Opis i dokaz ove teoreme može se naći u \cite{num_met}. Uslov kontrakcije zapisuje se sledećim izrazom:
\begin{equation}
	\norm{Bv_1-Bv_2}_{\infty} \leq \alpha \norm{v_1-v_2}_{\infty}
\end{equation}
za neko $\alpha \in [0, 1)$. 
Dokaz da je Belmanov operator kontrakcija dat je u nastavku:
\begin{align}
	\norm{Bv_1-Bv_2}_{\infty} &= \max_s |Bv_1(s)-Bv_2(s)| \\
	&= \mabs*{\max_{a}\sum_{s', r}^{} p(s', r~|~s,a)[r+\gamma v_1(s')] - \max_{a}\sum_{s', r}^{} p(s', r~|~s,a)[r+\gamma v_2(s')]} \\
	&\leq \max_{a} \mabs*{\sum_{s', r}^{} p(s', r~|~s,a)[r+\gamma v_1(s')] - \sum_{s', r}^{} p(s', r~|~s,a)[r+\gamma v_2(s')]} \\
	&=\max_{a} \mabs*{ \sum_{s', r}^{} p(s', r~|~s,a) \big[(r+\gamma v_1(s'))-(r+\gamma v_2(s'))\big]} \\
	&=\gamma\max_{a} \mabs*{ \sum_{s', r}^{} p(s', r~|~s,a)\big[v_1(s')-v_2(s')\big]} \\
	&\leq \gamma \max_s \mabs*{v_1(s)-v_2(s)} \\
	&= \gamma \norm{v1-v2}_{\infty}
\end{align}
Ukoliko je $\gamma \in [0, 1)$, Belmanov operator je kontrakcija. Treći red sledi iz činjenice da 
\begin{equation}
	\mabs*{\max_{x} f(x) - \max_{x} g(x)} \leq \max_{x}\mabs*{f(x)-g(x)}
\end{equation}
dok pretposlednji red sledi iz činjenice da je $p$ raspodela verovatnoće za fiksirane parametre $s$ i $a$. Pošto je dokazano da postoji jedinstvena fiksna tačka Belmanovog operatora i kako po njegovoj konstrukciji važi $Bv \geq v$, vidi se da je niz dobijen njegovom uzastopnom primenom rastući. Stoga je fiksna tačka zaista i optimalna funkcija vrednosti.
\par 
Ranije je objašnjeno kako se, ukoliko je poznata funkcija $v_*$, može konstruisati optimalna politika. Iz postojanja $v_*$ sledi postojanje $\pi_*$ ali ovakvih politika može biti više zbog mogućnosti da iz jednog stanja više akcija vode ka stanjima sa istom, maksimalnom, vrednošću. 
\par 
Iako sporiji postupak, traženje $q_*$ funkcije pogodnije je od traženja $v_*$ u slučajevima kada funkcija prelaska nije poznata. Pristupi traženju ovih funkcija dinamičkim programiranjem podjednako su zastupljeni. 
\par 
Funkciju vrednosti stanja i funkciju vrednosti akcije u stanju moguće je odrediti metodima linearnog programiranja. Jedan od primera je traženje $v_*$ na osnovu jednakosti \eqref{eq:bel_v_opt}, postavljanjem skupa uslova:
\begin{equation}
	v(s) \geq \sum_{s', r}^{} p(s', r~|~s,a)\big[r+\gamma v(s')\big] \text{,~za~sve~} s \in \mathcal{S} \text{~i~sve~} a \in \mathcal{A}(s)
\end{equation}
Ovih uslova ima $\sum_{s \in \mathcal{S}}^{} |\mathcal{A}(s)| \leq |\mathcal{S}||\mathbb{A}|$. Pri tim uslovima, treba minimizovati izraz $\sum_{s \in \mathcal{S}}^{} v(s)$. Rešavanje ovog problema daje $v_*$. Ovaj pristup rešavanju MDP-a pri određenim uslovima daje teorijski bržu konvergenciju od pristupa dinamičkim programiranjem. Uprkos tome, pri porastu broja stanja, pristup linearnim programiranjem brže postaje neizvodljiv od pristupa dinamičkim programiranjem.

\subsection{Učenje potkrepljivanjem u nepoznatom okruženju}

Do sada opisani pristupi rešavanju problema učenja potkrepljivanjem opisuju postupak u slučaju da su informacije o okruženju dostupne, odnosno da je funkcija prelaska, $p$, poznata. Međutim, u realnim situacijama ovo često nije slučaj. Učenje kada funkcija prelaska nije poznata naziva se učenjem u nepoznatom okruženju. U ovoj situaciji, na neki način neophodno je prikupljati podatke o okruženju. Pri učenju u nepoznatom okruženju, moguća su dva pristupa za učenje i skupljanje podataka o okruženju. Prvi podrazumeva praćenje neke politike i njeno konstantno unapređenje na osnovu signala dobijenog od okruženja. Ovo je pristup u skladu sa politikom (eng.~{\em on-policy}). Drugi pristup podrazumeva aproksimaciju optimalne politike dok se dela na osnovu neke druge politike. Naziv je pristup mimo politike (eng.~{\em off-policy}). Koji god pristup da se odabere, zbog toga što podataci o okruženju nisu dostupni, neophodno je održavati balans između iskorišćavanja već naučenog i israživanja. Pitanje koji se javlja prilikom traženja ovog odnosa naziva se dilema između istraživanja i iskorišćavanja (eng.~{\em exporation vs. exploitaion dilemma}) i prisutna je i u svakodnevnom životu. Čest pristup rešavanju ove dileme jeste $\varepsilon$-pohlepna politika, koja podrazumeva da se, za neko $\varepsilon \in (0,1]$, sve akcije sem najbolje u nekom stanju $s$ biraju sa verovatnoćom $\frac{\varepsilon}{|\mathcal{A}(s)|}$ dok se najbolja bira sa verovatnoćom $1- \varepsilon + \frac{\varepsilon}{|\mathcal{A}(s)|}$. Drugim rečima, sa verovatnoćom $1-\varepsilon$ bira se najbolja akcija u skladu sa nekom politikom a sa verovatnoćom $\varepsilon$ biće odabrana nasumična vrednost iz $\mathcal{A}(s)$. Metaparametar $\varepsilon$ često se menja u toku učenja.
\par 
Osnovni primeri metoda u skladu sa politikom i metoda mimo politike u nepoznatom okruženju su, redom, Sarsa i Q učenje. Metod Sarsa podrazumeva učenje politike implicitno, tako što se iterativno unapređuje funkcija vrednosti akcije u stanju koja joj odgovara. Za izvršavanje ovog algoritma neophodni su sledeći podaci u jednom vremenskom trenutku, $t$: stanje u kome se na početku agent nalazi, $s_t$, akcija koja je preduzeta u tom stanju, $a_t$, nagrada i stanje koji zauzvrat dobijeni, $r_{t+1}$ i $s_{t+1}$ i akcija koja se u novom stanju preduzima, $a_{t+1}$. Ovi podaci čine petorku, $(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1})$, odakle potiče ime metoda. Pravilo ažuriranja tekuće aproksimacije $q$ funkcije je:
\begin{equation}
	\begin{aligned}
		q(s_t, a_t) \leftarrow& ~q(s_t, a_t) + \alpha [ R_{t+1} + \gamma q(s_{t+1}, a_{t+1}) - q(s_t, a_t)] \\
					=&~ (1-\alpha)q(s_t, a_t) + \alpha [ R_{t+1} + \gamma q(s_{t+1}, a_{t+1}) ]
	\end{aligned}
\end{equation}
Simbol $\leftarrow$ označava da je nova vrednost za $q(s_t, a_t)$ dobijena od tekuće datim izrazom,\footnote{Nisu korišćeni izrazi oblika $q_k(s_t, a_t)$, koji označavaju aproksimaciju u $k$-toj iteraciji jer jedno ažuriranje utiče na vrednost samo za jedan par $s_t, a_t$.} dok je $\alpha$ stopa učenja, metaparametar čija je uloga da odredi koliku težinu imaju nove vrednosti u odnosu na stare, kao što se vidi iz druge jednakosti. Stopa učenja ne mora biti fiksna.
Petorke $(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1})$ prikupljaju se u skladu sa politikom određenom trenutnom aproksimacijom $q$. Uz određene uslove vezane za $\varepsilon$ i stopu učenja, ovaj postupak dovešće do konvergencije ka optimalnoj funkciji vrednosti akcije u stanju.
\par 
Predstavnik učenja mimo politike je takozvano Q učenje (eng.~{\em Q-learning}). Ovaj algoritam podrazumeva poboljšavanje tekuće aproksimacije uz pretpostavku da će se u narednom stanju odabrati najbolja akcija, nezavisno od toga koja će akcija zaista biti preduzeta. Pravilo ažuriranja je:
\begin{equation}
	\label{eq:q_ucenje}
		q(s_t, a_t) \leftarrow ~q(s_t, a_t) + \alpha [ R_{t+1} + \gamma \max_a q(s_{t+1}, a) - q(s_t, a_t)]
\end{equation}
Algoritam Q učenje garantuje konvergenciju tekuće aproksimacije, $q$, ka optimalnoj funkciji vrednosti akcije u stanju, $q_*$, ako se vrednosti za sve parove $s, a$ stalno ažuriraju. Na ovom algoritmu zasnovan je DQN, opisan kasnije.
\par 
U oba algoritma se vrednosti za $q(s,a)$ inicijalizuju na nasumično odabrane vrednosti, sem za završna stanja, gde se, nezavisno od akcije, vrednost $q$ funkcije postavlja na $0$. Takođe, politika se ne konstruiše tako da se u svakom stanju $s$ bira akcija koja maksimizuje $q(s,a)$, odnosno na pohlepan način, već se uvek uključuje element istraživanja. Često se od trenutne aproksimacije $q$ konstruiše $\varepsilon$-pohlepna politika. 
\par 
Na prvi pogled, ova dva algoritma deluju jako slično. U slučaju da ne postoji element istraživanja, odnosno ako se od trenutne aproksimacije $q$ konstruise politika na pohlepan način, tada su i isti. Međutim, element istraživanja uvek postoji jer, u suprotnom, do učenja ne bi došlo. Glavna razlika između Sarsa algoritma i Q učenja jeste u tome što u se prvom algoritmu ažuriranje vrši na osnovu akcije koja će se zaista preduzeti praćenjem politike, dok će se u drugom algoritmu ažuriranje uvek vršiti optimistično, kao da će se praćenjem politike uvek odabrati akcija koja maksimizuje $q(s,a)$ po $a$, što nije nužno tačno. 
\par 
Svi navedeni metodi prestaju da budu izvodljivi kada MDP dostigne određenu veličinu. Ovo se, na primer, može desiti ukoliko je prostor stanja neprekidan ali je urađena diskretizacija ili ako je sam prostor diskretan ali izuzetno veliki. U velikim prostorima može se desiti da su dva ili više stanja suštinski ista ali do sada opisani algoritmi nisu u stanju to da zaključe. Uz to, može se desiti da postoje stanja u koja je nemoguće doći. Prethodno opisani algoritmi oslanjaju se na to da će se stanja iznova posećivati ali i ovo postaje problem u velikim prostorima. U cilju rešavanja ovih problema, pribegava se korišćenjem nekih od modela nadgledanog učenja radi aproksimacije funkcije vrednosti stanja ili akcije u stanju. Nadgledano učenje zahteva skup unapred uparenih ulaznih i izlaznih vrednosti ali, kao što je već pomenuto, u učenju potkrepljivanjem ove informacije nisu dostupne. Stoga je neophodno izvršiti određene izmene algoritama učenja poktrepljivanjem. Jedan primer ove kombinacije pristupa je DQN, opisan u nastavku.

%Još jedan način rešavanja MDP-a predstavljaju takozvani Monte Karlo metodi, koji se oslanjaju na prikupljanje %informacija o okruženju iz iskustva. Na primer, za približno određivanje optimalne funkcije vrednosti stanja %čuva se prosek vrednosti dobijenih nakon dolaska u to stanje. Proširivanje ovog iskustva [u ovom slučaju] %znači bolje određivanje optimalne funkcije vrednosti.

% ostali metodi resavanja problema RL
% neka prica o opstem Q ucenju
% neki zakljucak -- kada je sta bolje koristiti




