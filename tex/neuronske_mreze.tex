\chapter{Neuronske mreže}
\label{ch:nn}

Neuronske mreže (eng.~{\em neural networks}) predstavljaju danas izuzetno popularan vid mašinskog učenja. Ovi modeli izuzetno su fleksibilni i imaju široku primenu.  Koriste se za prepoznavanje govora, prevođenje, prepoznavanje oblika na slikama, upravljanje vozilima, uspostavljanje dijagnoza u medicini, igranje igara itd. Pun naziv je veštačka neuronska mreža (eng.~{\em artificial neural network}, skr.~{\em ANN}) jer se ovakvi modeli idejno zasnivaju na načinu na koji mozak funkcioniše. Ipak, neuronske mreže ne predstavljaju vernu kopiju mozga. Osnovne gradivne jedinice, neuroni, zasnovani su na neuronima u mozgu, dok veze između njih predstavljaju sinapse.\footnote{Sinapsa je biološka struktura koja omogućuje komunikaciju između neurona.} Te veze opisuju odnose između neurona i obično im se dodeljuje numerička težina.

\par
Postoji nekoliko različitih vrsta neuronskih mreža. Tipičan primer jesu neuronske mreže sa propagacijom unapred. Ime proističe iz činjenice da podaci teku od ulaza mreže do izlaza, bez postojanja ikakve povratne sprege. Neuronske mreže sa propagacijom unapred sastoje se iz slojeva neurona. Ukoliko se u ovaj model uvede neki tip povratne sprege, tada se govori o rekurentnim neuronskim mrežama. Pri radu sa slikama i raznim drugim vrstama signala, najčešće se koriste konvolutivne neuronske mreže, o kojima će biti reči kasnije. Ono što je zajedničko je da su neuronske mreže sposobne za izdvajanje određenih karakteristika u podacima koji se obrađuju. To znači da se vrši kreiranje novih atributa na osnovu već postojećih atributa ili direktno iz ulaznih podataka. Taj proces naziva se ekstrakcijom atributa i smatra se da je to jedan od najbitnijih razloga za delotvornost neuronskih mreža.
\par
Za uspeh neuronskih mreža zaslužna je njihova fleksibilnost, ali se rezultati najčešće dobijaju eksperimentisanjem. Naime, veliki deo zaključaka o ponašanju neuronskih mreža u raznim situacijama nije teorijski potkrepljen. Stoga, istraživački rad vezan za neuronske mreže zahteva dosta pokušaja da bi se došlo do uspeha.

\section{Neuronske mreže sa propagacijom unapred}

Neuronske mreže sa propagacijom unapred jedna su od najkorišćenijih vrsta neuronskih mreža. Gradivni elementi ovakvog modela, neuroni (koji se još nazivaju i jedinicama), organizuju se u slojeve koji se nadovezuju i time čine neuronsku mrežu. Organizacija neurona i slojeva, uključujući i veze između neurona, predstavlja arhitekturu mreže. Prvi sloj mreže naziva se ulaznim slojem dok se poslednji sloj naziva izlaznim slojem. Neuroni prvog sloja kao argumente primaju ulaze mreže dok neuroni svakog od preostalih slojeva kao svoje ulaze prihvataju izlaze prethodnog sloja. Svi slojevi koji svoje izlaze prosleđuju narednom sloju nazivaju se skrivenim slojevima. Mreže sa više od jednog skrivenog sloja nazivaju se dubokim neuronskim mrežama.  Broj slojeva mreže određuje njenu dubinu. Termin duboko učenje nastao je baš iz ove terminologije.
\par
Svaki neuron opisuje se pomoću vektora $w = (w_0, ..., w_n)$ koji se naziva vektorom težina. Ulazni vektor $x = (x_1, ..., x_n)$ linearno se transformiše na sledeći način:
\begin{equation}
\label{eq:neuron}
		w_0 + \sum_{i=1}^{n} x_nw_n 
\end{equation}
a zatim se primenjuje takozvana aktivaciona funkcija, $g$. Izlaz iz neurona je 
\begin{equation}
	g(w_0 + \sum_{i=1}^{n} x_nw_n)
\end{equation}
i, uprkos linearnosti prve transformacije, izlaz ne mora biti linearna transformacija ulaza, tj. $g$ najčešće nije linearna funkcija. Za $g$ se bira nelinearna funkcija jer se u suprotnom kao celokupna transformacija koju neuron vrši dobija linearna funkcija; na ovaj način, mreža bi predstavljala linearnu funkciju i ne bi bilo moguće njom aproksimirati nelinearne funkcije dovoljno dobro. Vrednost $w_0$ naziva se slobodnim članom. Nekada se vektor $x$ proširuje tako da bude oblika $x = (1, x_1, ..., x_n)$ kako bi izraz \eqref{eq:neuron} imao kraći zapis $f_w(x) = w \cdot x$, gde $\cdot$ označava skalarni proizvod.\\

\begin{figure}
	\centering
	\resizebox{.5\linewidth}{!}{\input{img/neuron.tikz}}
	\caption{Neuron}
	\label{fig:neuron}
\end{figure}

\begin{figure}
	\centering
	\resizebox{.5\linewidth}{!}{\input{img/mreza.tikz}}
	\caption{Neuronska mreža koja sadrži jedan skriveni sloj}
	\label{fig:mreza}
\end{figure}

Model, tj. neuronska mreža, formalno se definiše na sledeći način:
\begin{equation}
	\label{eq:nn}
	\begin{gathered}
		h_0 = x  \\
	 	h_i = g_i(W_ih_{i-1} + w_{i0}) \text{,~za~} i=1, ..., L
	\end{gathered}
\end{equation}
gde je $x$ vektor ulaza u mrežu predstavljen kao kolona, $W_i$ je matrica čija $j$-ta vrsta predstavlja vektor težina $j$-tog neurona u sloju $i$ a $w_{i0}$ je kolona slobodnih članova svih jedinica u sloju $i$. Funkcije $g_i$ su nelinearne aktivacione funkcije i za vektor $t=(t_1, ..., t_n)$, $g_i(t)$ predstavlja kolonu $(g_i(t_1), ..., g_i(t_n))^T$. Na ovaj način dobija se funkcija čiji su parametri $W_i$ i $w_{i0}$ za $i=1,...,L$. Ako se parametri označe sa $w$, tada se model zapisati kao $f_w$. Parametri $w$ mogu se pronaći matematičkom optimizacijom nekog kriterijuma kvaliteta modela. Taj proces opisan je u delu ~\ref{subsec:optimizacija}.

\subsection{Aktivacione funkcije}

Preteča neuronskih mreža, perceptron, je model koji se sastoji samo iz jednog neurona čija je aktivaciona funkcija data sledećim izrazom:
\begin{equation}
	g(x)=
	\begin{cases}
		1, 	& \text{~ako~} x \geq 0 \\
		0, 	& \text{~inače}
	\end{cases}
\end{equation} 
\par
Definicija aktivacione funkcije perceptrona znači da njegova primena ima relativno jako ograničenje. Izvod ove funkcije je $0$ u svim tačkama sem u $x=0$, gde izvod ne postoji. To znači da ovakva funkcija nije pogodna za upotrebu uz optimizaciju metodom gradijentnog spusta, koji se oslanja na korišćenje izvoda funkcije.
\par
Dakle, neophodno je naći druge funkcije koje služe kao aktivacione funkcije. Poželjna svojstva aktivacione funkcije su:
\begin{itemize}
	\item Nelinearnost: Kao što je objašnjeno ranije, kompozicija linearnih funkcija daje linearnu funkciju, što onemogućuje dovoljno preciznu aproksimaciju nelinearnih funkcija;
	\item Diferencijabilnost: Optimizacija se najčešće vrši metodima koji koriste gradijent funkcije;
	\item Monotonost: Ako aktivaciona funkcija nije monotona, povećavanje nekog od težinskih parametara neurona, umesto da poveća izlaz i time proizvede jači signal, može imati suprotan efekat;
	\item Ograničenost: Ukoliko vrednosti unutar neuronske mreže nisu ograničene, moguće je da dođe do pojavljivanja ogromnih vrednosti koje potencijalno dovode do prekoračenja. Ograničene aktivacione funkcije znatno ublažuju ovaj problem. 
\end{itemize}
Dozvoljeno je da aktivaciona funkcija ne poseduje neko od navedenih svojstava.
\par
Najčešće korišćene aktivacione funkcije su:
\begin{itemize}
	\item Sigmoidna funkcija: $\sigma (x) = \cfrac{1}{1+e^{-x}}$
	\item Tangens hiperbolički: $tanh(x) = \cfrac{e^{2x}-1}{e^{2x}+1}$
	\item Ispravljena linearna jedinica: $ReLU(x) = max(0, x)$
\end{itemize}

Sigmoidna funkcija bila je najkorišćenija aktivaciona funkcija pri radu sa neuronskim mrežama. Ograničena je (sve slike nalaze se u intervalu $(-1, 1)$), monotona i diferencijabilna u svakoj tački skupa $\mathbb{R}$. Međutim, što se argument više udaljava od nule, to nagib funkcije postaje manji. To znači da će gradijent funkcije biti mali i da će učenje teći jako sporo. 
\par 
Tangens hiperbolički srodan je sigmoidnoj funkciji ($tanh(x) = 2\sigma (x) - 1$), ali je imao veći uspeh od sigmoidne funkcije.  U okolini nule, ova funkcija slična je identičkoj, što olakšava učenje. Međutim, i pri korišćenju ove funkcije može se naići na problem sa malim gradijentima ukoliko se argument dovoljno udalji od nule. 
\par 
Uprkos tome što za razliku od prethodne dve funkcije nije ni ograničena ni diferencijabilna u svim tačkama domena, danas je ispravljena linearna jedinica najpopularniji izbor za aktivacionu funkciju. Funkcija je jednaka identitetu desno od nule i stoga se gradijent ne menja. Takođe, verovatnoća da se traži gradijent u tački u kojoj funkcija nije diferencijabilna je mala. Ipak, ni ova funkcija nije bez mana; problem često pravi deo levo od nule, gde je funkcija konstantna. To znači da je gradijent nula i da se prilikom optimizacije težine neurona neće izmeniti. Zbog nedostatka promene, može se desiti da neki neuroni u mreži postanu pasivni, tj. da im izlaz postane $0$. Za ovaj problem postoje rešenja; jedno jeste da izlaz funkcije desno od nule ne bude konstanta $0$ već $\alpha x$, za neko malo $\alpha$. Ta modifikovana ReLU funkcija naziva se nakošena ispravljena linearna jedinica (eng.~{\em leaky rectified linear unit}). 
\par 
Iako sve ove funkcije imaju prednosti i mane u odnosu na preostale, ne postoji jedinstveni izbor nego je na osnovu problema neophodno zaključiti koju je aktivacionu funkciju najbolje koristiti.

\subsubsection{Izlazni sloj}

Neuronske mreže koriste se pri regresiji, određivanju funkcije koja opisuje vezu izmedju ulaza i izlaza, i klasifikaciji, svrstavanje ulaznih vektora u jednu od konačnog broja kategorija. Pri regresiji, u poslednjem sloju ne primenjuje se aktivaciona funkcija. Proces optimizacije svodi se na minimizaciju funkcije greške. Kod rešavanja problema klasifikacije (u $N$ kategorija), koristi se funkcija mekog maksimuma (eng.~{\em softmax}):
\begin{equation}
	softmax(x) = \bigg( \cfrac{e^{x_1}}{\sum_{i=1}^{N} e^{x_i}}, ~...~, \cfrac{e^{x_N}}{\sum_{i=1}^{N} e^{x_i}} \bigg)
\end{equation}
Suma ovako dobijenog vektora je $1$ i stoga može predstavljati diskretnu raspodelu verovatnoća. Za vrednost aproksimacije uzima se kategorija kojoj odgovara najviša vrednost izlaznog vektora. Za optimizaciju pri radu sa probabilističkim problemima, kao što je problem klasifikacije, primenjuje se metod maksimalne verodostojnosti (eng.~{\em maximum likelihood estimate}), odnosno traži se maksimum sledećeg izraza: 
\begin{equation}
	P(y_1, ..., y_N | x_1, ..., x_N)
\end{equation}.

\subsection{Optimizacija}
\label{subsec:optimizacija}

Ukoliko je neuronska mreža predstavljena kao funkcija $f_w$, gde su $w$ parametri mreže, neophodno je izvršiti minimizaciju\footnote{Naravno, optimizacioni metodi primenjuju se i na maksimizaciju, ali je u slučaju mašinskog učenja najčešće neophodno minizovati funkciju greške.} funkcije koja predstavlja kriterijum kvaliteta aproksimacije. Problem optimizacije u slučaju neuronskih mreža težak je zbog nekonveksnosti. Ona čini neke metode teško primenljivim ili izuzetno sporim. Moguće je i završiti u lokalnom optimumu funkcije. Uobičajeno se koriste metodi zasnovani na gradijentu funkcije. Postoje metodi drugog reda, zasnovani na hesijanu,\footnote{Hesijan je matrica parcijalnih izvoda drugog reda.} ali je njegovo računanje u slučaju većeg broja parametara preskupo. \par
Učenje funkcioniše na sledeći način za fiksirane ulaze $x$ posmatra se njima uparen izlaz $y$ i $f_w(x)$ a zatim i $L(y, f_w(x))$, odnosno funkcija greške između stvarne i očekivane vrednosti. Koristeći algoritam propagacije unazad (\ref{subsub:backprop}) uz neki od algoritama za optimizaciju, vrši se minimzacija funkcije $L$ u odnosu na parametre mreže, $w$. 
%Jedna od najčešće korišćenih funkcija greške pri regresiji je srednjekvadratna greška:
%\begin{equation}
%	L(y, \hat{y}) =\frac{1}{2}\norm{y-\hat{y}}_2^2 = \frac{1}{2}\sum_{i=1}^{2} (y_i - \hat{y}_i)^2
%\end{equation}
%Kako su pri minimizaciji $L$ za određeni par $x,y$ jedino težine nepoznate, u nastavku će %$L(w)$ označavati funkciju greške za neki par.

\subsubsection{Metod gradijentnog spusta i stohastičkog gradijentnog spusta}

Gradijent funkcije $f:\mathbb{R}^n \rightarrow \mathbb{R}$ u tački $x=(x_1, ..., x_n)$ označava se sa $\nabla f$ i predstavlja vektor parcijalnih izvoda u toj tački:
\begin{equation}
\nabla f(x) = \bigg( \cfrac{\partial f}{\partial x_1}(x),...,\cfrac{\partial f}{\partial x_n}(x) \bigg)
\end{equation}

Gradijent funkcije u tački $x$ predstavlja pravac i smer najbržeg rasta funkcije pa $- \nabla f(x)$ predstavlja pravac smer najbržeg opadanja funkcije. Kako se najčešće minimizuje funkcija greške, u oznaci $L$, na dalje je korišćeno to ime za funkciju umesto $f$.
\par 
Metod gradijentnog spusta jedan je od najstarijih metoda  optimizacije. Iterativnim pristupom minimizuje se konveksna diferencijabilna funkcija. Polazeći od nasumično odabrane tačke i prateći pravac i smer gradijenta u svakom koraku, dolazi se do minimuma funkcije. Iterativni korak definisan je na sledeći način:
\begin{equation}
	\label{eq:gradijentni_spust}
	w_{k+1} = w_k - \alpha_k \nabla L(w_k),~ k=0, 1, 2, ... ,
\end{equation}
gde je $w_0$ ta nasumično odabrana početna tačka a $\alpha_k$ je pozitivan realan broj koji se naziva veličinom koraka ili stopom učenja (eng.~{\em learning rate}). 
Za funkciju greške u ovom slučaju uzima se srednjekvadratno odstupanje:
\begin{equation}
	\frac{1}{2}\sum_{i=1}^{N} (y_i - f_w(x_i))^2
\end{equation}

Bitno je pažljivo odabrati veličinu koraka jer ova vrednost može uticati na konvergenciju. Jedan primer odabira veličine koraka jeste niz za koji važe Robins Monroovi uslovi:\footnote{\url{https://en.wikipedia.org/wiki/Stochastic_approximation}}
\begin{equation}
	 \sum_{k=0}^{\infty} \alpha_k = \infty \hspace{2cm} \sum_{k=0}^{\infty} \alpha_k^2 < \infty 
	\end{equation}
Jednostavniji pristup bio bi da se odabere mali pozitivan parametar $\alpha$ i da za svako $k$ važi $\alpha_k = \alpha$.
\par
Postavlja se i pitanje koliko koraka načiniti pre zaustavljanja. U praksi se koristi nekoliko kriterijuma kao što su zaustavljanje kada su dve uzastopne vrednosti $w_k$ i $w_{k+1}$ dovoljno bliske, kada su vrednosti funkcije za dve uzastopne vrednosti dovoljno bliske, ali se može zaustaviti i nakon unapred određenog broja koraka. Postoji još kriterijuma i moguće ih je kombinovati.
\par
Iako jednostavan i široko primenljiv metod optimizacije, gradijentni spust nije najbolji izbor. Naime, pravac najbržeg uspona funkcije nije uvek i pravac koji osigurava najbrže približavanje optimumu funkcije. U praksi, gradijentni spust ume da proizvodi cik-cak kretanje koje dovodi do spore konvergencije. Takođe, za jedan iterativni korak neophodno je proći kroz sve parove ulaza i izlaza, što u slučaju velikog skupa podataka za obučavanje može biti jako velika količina podataka.
\par
Za obučavanje neuronskih mreža češće se koristi metod stohastičkog gradijentnog spusta. Pretpostavka je da je funkcija koja se optimizuje oblika:

\begin{equation}
		L(w) = \frac{1}{N}\sum_{i=1}^{N} L_i(w)	
\end{equation}
odnosno da se može predstaviti kao prosek nekih $N$ funkcija. Kako je neuronska mreža jedan od metoda mašinskog učenja, na raspolaganju je skup za obučavanje pa se funkcija greške na celom skupu može predstaviti kao prosek grešaka na pojedinačnim instancama skupa. Novi oblik 
jednakosti ~\eqref{eq:gradijentni_spust} je:
\begin{equation}
	w_{k+1} = w_k - \alpha_k \bigg( \frac{1}{N}\sum_{i=1}^{N} \nabla L_i(w_k) \bigg)\text{,~} k=0,1,2, ... 
\end{equation}
Pri korišćenju stohastičkog gradijentnog spusta za minimizaciju funkcije greške, iterativni korak izgleda ovako:
\begin{equation}
	w_{k+1} = w_k - \alpha \nabla L_i(w_k) 
\end{equation}
Postoje razni načini za odabir vrednosti $i$ u nekom koraku, kao što je $i=k (mod~N) + 1$, gde je $N$ veličina skupa za obučavanje. Još jedan primer je nasumični odabir instance u svakom koraku. Kakav god način izbora bio, neophodno je iskoristiti sve greške. Moguće je proći greške iz skupa za obučavanje i nekoliko puta dok se ne postigne željeni nivo aproksimacije. 
\par
Kako ova aproksimacija može biti prilično neprecizna, pribegava se kompromisu: prilikom iterativnog koraka ne koriste se pojedinačne instance već neki podskup skupa za obučavanje (eng.~{\em minibatch}) i umesto greške na pojedinačnoj instanci koristi se prosek grešaka na odabranom podskupu. Pri treniranju neuronskih mreža, ovo je uobičajeni pristup.
\par
Metod stohastičkog gradijentnog spusta manje je računski zahtevan od gradijentnog spusta, ali je manje precizan i neophodan je veći broj iteracija kako bi se dostigao minimum.
\par
Postoje razni metodi optimizacije koji se koriste pri mašinskom učenju. Neki menjaju veličinu koraka u zavisnosti od prethodnih izračunatih koraka i gradijenata. Takvi metodi nazivaju se adaptivnim metodima optimizacije. Primer adaptivnih metoda optimizacije su Adam i RMSProp.

\subsubsection{RMSProp}

Algoritam RMSProp (eng.~{\em root mean square propagation}) predložio je Džefri Hinton (eng.~{\em Geoffrey Hinton}) na jednom od svojih predavanja na sajtu Kursera.\footnote{\url{http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf}} Ovo je algoritam optimizacije korišćen prilikom razvijanja DQN algoritma. Glavna ideja je čuvanje dosadašnjeg otežanog proseka kvadrata gradijenta funkcije koji će biti obeležen sa $g_k$. Simbol $\odot$ obeležava pokoordinatno množenje dva vektora. Kako algoritam nije objavljen u radu, može se naći veliki broj implementacija. U nastavku je predstavljen algoritam u skladu sa implementacijom iz biblioteke Keras, koja je korišćena za implementaciju DQN algoritma u ovom radu.
\begin{equation}
	\begin{gathered}
		g_0 = 0 \\
		\alpha_0 = \alpha \\
		g_{k+1} = \gamma g_k + (1 - \gamma)\nabla L(w_k) \odot \nabla L(w_k) \\
		\alpha_{k+1} = \frac{\alpha_k}{1+d(k+1)}	
	\end{gathered}
\end{equation}
Tada se iterativni korak definiše: 
\begin{equation}
	w_{k+1} = w_k - \frac{\alpha_{k+1}}{\sqrt{g_{k+1}} + \varepsilon} \nabla L(w_k)
\end{equation}
Sve operacije vrše se pokoordinatno. Parametar $\gamma$ pripada poluotvorenom intervalu $\left[0, 1\right) $. U svom predavanju, Hinton predlaže da njegova vrednost bude $0.9$. Preporučena vrednost za veličinu koraka odnosno stopu učenja, u oznaci $\alpha$, je $0.001$ dok $d$ označava faktor opadanja za parametar $\alpha$. Parametar $\varepsilon$ služi da bi se izbeglo deljenje nulom i obično je reda veličine $10^{-8}$.

\subsubsection{Adam}

Adam (eng.~{\em adaptive moment estimation}) jedan je od najčešćih algoritama za optimizaciju korišćen pri obučavanju neuronskih mreža. Algoritam Adam zasnovan je na korišćenju ocena prvog i drugog momenta gradijenta, datim sledećim formulama:

\begin{equation}
	\begin{gathered}
			m_0 = 0 \\
			v_0 = 0 \\ 
			m_{k+1} = \beta_1 m_k + (1-\beta_1) \nabla L(w_k) \\
			v_{k+1} = \beta_2 v_k + (1-\beta_2) \nabla L(w_k) \odot \nabla L(w_k) 
	\end{gathered}
\end{equation}

Ocena prvog momenta, $m_0$, predstavlja otežani prosek pravca kretanja dok ocena drugog momenta, $v_0$, predstavlja otežani prosek kvadrata norme gradijenata. Međutim, ove dve ocene su pristrasne ka početnoj vrednosti, u ovom slučaju $0$.\footnote{Ovde se misli na $0$ vektor istih dimenzija kao $x_k$ u slučaju prvog momenta i skalar $0$ u slučaju drugog momenta} Da bi se to ispravilo, vrši se sledeća korekcija:
\begin{equation}
	\begin{gathered}
		\hat{m}_{k+1} = \frac{m_{k+1} }{1 - \beta_1^{k+1}} \\
		\hat{v}_{k+1} = \frac{v_{k+1} }{1 - \beta_2^{k+1}}
	\end{gathered}
\end{equation}
Na kraju, iterativni korak dat je ispod. Dodavanje skalara $\varepsilon$ na vektor $\hat{v}_{k+1}$ predstavlja dodavanje tog skalara svakom članu datog vektora. Korenovanje, deljenje i oduzimanje vrše se pokoordinatno.
\begin{equation}
	w_{k+1} = w_k - \alpha \frac{\hat{m}_{k+1}}{\sqrt{\hat{v}_{k+1}} + \varepsilon}
\end{equation}

Parametar $\alpha$ naziva se veličina koraka ili stopa učenja. Vrednosti parametara $\beta_1$ i $\beta_2$ ograničene su na skup $\left[0, 1\right) $ i preporučene vrednosti su $0.9$ i $0.999$, redom, dok se za $\varepsilon$ preporučuje vrednost $10^{-8}$. Kao i u algoritmu RMSProp, svrha parametra $\varepsilon$ je izbegavanje deljenja sa nulom. Takođe nalik algoritmu RMSProp opisanom iznad, moguće je uvesti stopu opadanja parametra $\alpha$.
\par
Intuicija kojom se vodi algoritam Adam jeste da dužina svakog koraka zavisi od osobina funkcije u regionu u kom se trenutno vrši optimizacija. Ovaj algoritam u mnogim primenama pokazao se kao superioran u odnosu na ostale algoritme za optimizaciju.



\subsubsection{Metod propagacije unazad}
\label{subsub:backprop}

Do sada je objašnjeno kako iskoristiti gradijent funkcije radi nalaženja odgovarajućih parametara funkcije. Međutim, računanje gradijenta u slučaju neuronskih mreža izuzetno je zahtevan proces. U ovu svrhu koristi se metod propagacije unazad (eng.~{\em back propagation}). Prilikom obučavanja mreže, cilj je minimizovati funkciju greške, $L(w)$, gde $w$ predstavlja parametre mreže. Ulazi i izlazi mreže ne nalaze se u ovom zapisu jer su u konkretnom slučaju fiksirani i neophodno je izvršiti izmenu parametara mreže tako da funkcija greške bude što manja. 
\par 
Iz jednakosti \eqref{eq:nn} jasno je da neuronska mreža predstavlja složenu funkciju a, kako je funkcija $L$ mera odstupanja vrednosti koje mreža predviđa i željenih vrednosti, tada je i $L$ složena funkcija. Izvod složene funkcije $g \circ h$ računa se na sledeći način:
\begin{equation}
	(g \circ h)' = (g' \circ h)h'
\end{equation}
Ovo pravilo može se primeniti i kada kompoziciju čine više od dve funkcije.
\par 
Kako neuronske mreže skoro uvek sadrže više od jednog parametra, koristi se pravilo za računanje parcijalnog izvoda složene funkcije više promenljivih. Neka su date funkcije $h:\mathbb{R}^m\rightarrow\mathbb{R}^k$ i $g:\mathbb{R}^k\rightarrow\mathbb{R}$. Tada se parcijalni izvod funkcije $g \circ h$ po $i$-toj promenljivoj računa koristeći sledeće pravilo:
\begin{equation}
	\partial_i(g \circ h) = \sum_{j=1}^{k}(\partial_j g \circ h) \partial_i h_j	
\end{equation}
gde $h_j$ označava $j$-tu komponentu funkcije $h$ a $\partial_j g$ parcijalni izvod funkcije $g$ po $j$-tom argumentu. Metod propagacije unazad koristi ovo pravilo za izračunavanje parcijalnih izvoda po svim parametrima mreze. Počinje se od izlaznog sloja i kreće se ka ulaznom, odakle potiče ime metoda. Čuva se do sada akumulirani izvod funkcije. Za svaki sloj računa se izvod po parametrima tekućeg sloja i dosadašnji akumulirani izvod proširuje se izvodom tekućeg sloja. Kako jedan sloj predstavlja linearnu kombinaciju izlaza prethodnog sloja i parametara mreže na koju je primenjena aktivaciona funkcija, neophodno je računati izvode i aktivacione funkcije i te linearne kombinacije.
\par 
Sada su dati svi alati za optimizaciju neuronske mreže sa propagacijom unapred. Treba imati u vidu da proces obučavanja velikih neuronskih mreža može biti izuzetno skup. Takođe, na sam proces učenja mogu uticati razni faktori kao što su arhitektura mreže, podela podataka na skupove za obučavanje i testiranje ili parametri algoritma za optimizaciju. Ovi faktori nazivaju se metaparametrima (eng.~{\em hyperparameter}) i neretko je neophodno isprobati razne njihove kombinacije dok ponašanje mreže ne dostigne željeni nivo. Često se umesto traženja metaparametara pribegava korišćenju unapred ispitanih vrednosti za koje je već pokazano da daju željene rezultate pri rešavanju nekog problema.


%\par 
%Ovaj metod danas je široko korišćen, najpre za obučavanje neuronskih mreža ali ima i druge primene kao što su...


% $f: \mathbb{R} \rightarrow \mathbb{R}$
%\frac{\partial (g \circ h)}{\partial w_i} = \sum_{j=1}^{k} \frac{\partial g}{\partial h_j} \frac{\partial h_j}{\partial w_i} 



\subsection{Prednosti i mane}

Neuronske mreže pokazale su se kao jako korisne za rešavanje praktičnih problema zbog svoje izuzetne fleksibilnosti. Međutim, za proces obučavanja neuronske mreže neophodno je imati veliku količinu podataka. Proces učenja može biti izuzetno spor, posebno ukoliko se uvede isprobavanje raznih vrednosti metaparametara. Velika fleksibilnost može izazvati i preprilagođavanje podacima i time učiniti performanse mreže nad novim podacima lošim. Postoje i problemi pri optimizaciji kao što su takozvani problemi nestajućih i eksplodirajućih gradijenata. Iako su u stanju da konstruišu nove atribute na osnovu starih, struktura obučene mreže nije čitljiva za ljude. U nekim situacijama, ovo može izazvati probleme. Na primer, ukoliko klijent podnese zahtev za kredit i neuronska mreža odluči da nije podoban, nije moguće objasniti razlog odbijanja. Neuronske mreže takođe su dosta računski i razvojno zahtevne. Nekada će neki već poznat algoritam dati zadovoljavajuće rešenje dok razvoj neuronske mreže može biti skup i po pitanju vremena razvijanja sistema i po pitanju kasnijeg rada sistema. 
Kako ne postoje teorijske smernice za rad sa neuronskim mrežama, odluke vezane za razvoj sistema neophodno je donositi empirijski.

\section{Konvolutivne neuronske mreže}
\label{sec:cnn}




% BACKPROP
% CNN