\chapter{Neuronske mreže}
\label{ch:nn}

Neuronske mreže (eng. neural networks) predstavljaju danas izuzetno popularan vid mašinskog učenja. Ovi modeli izuzetno su fleksibilni i imaju široku primenu.  Koriste se za prepoznavanje govora, prevođenje, prepoznavanje oblika na slikama, upravljanje vozilima, uspostavljanje dijagnoza u medicini, igranje igara itd. [Neuronskim mrežama može se aproksimirati proizvoljna neprekidna funkcija.] Pun naziv je veštačka neuronska mreža (eng. artificial neural network, skr. ANN) jer se ovakvi modeli idejno zasnivaju na načinu na koji mozak funkcioniše. Osnovne gradivne jedinice, neuroni, zasnovani su na neuronima u mozgu, dok veze između njih predstavljaju sinapse\footnote{Sinapsa je struktura koja omogućuje komunikaciju između neurona.}. Te veze opisuju odnose između neurona i obično im se dodeljuje numerička težina.

\par
Postoji nekoliko različitih vrsta neuronskih mreža. Tipičan primer jesu neuronske mreže sa propagacijom unapred. Ime proističe iz činjenice da podaci teku od ulaza mreže do izlaza, bez postojanja ikakve povratne sprege. Neuronske mreže sa propagacijom unapred sastoje se iz slojeva neurona, osnovnih gradivnih jedinica. Ukoliko se u ovaj model uvede neki tip povratne sprege, tada se govori o rekurentnim neuronskim mrežama. Pri radu sa slikama i raznim drugim vrstama signala, najčešće se koriste konvolutivne neuronske mreže, o kojima će biti reči kasnije. Ono što je zajedničko je da su neuronske mreže sposobne za izdvajanje određenih karakteristika u podacima koji se obrađuju. To znači da se vrši kreiranje novih atributa na osnovu već postojećih. Ovo se naziva ekstrakcijom atributa i smatra se da je to jedan od najbitnijih razloga za delotvornost neuronskih mreža.

\section{Neuronske mreže sa propagacijom unapred}

Neuronske mreže jedna su od najkorišćenijih vrsta neuronskih mreža. Gradivni elementi ovakvog modela, neuroni (koji se još nazivaju i jedinicama), organizuju se u slojeve koji se nadovezuju i time čine neuronsku mrežu. Organizacija neurona i slojeva predstavlja arhitekturu mreže. Prvi sloj mreže naziva se ulaznim slojem dok se poslednji sloj naziva izlaznim slojem. Neuroni prvog sloja kao argumente primaju ulaze mreže dok neuroni svakog od preostalih slojeva kao svoje ulaze prihvataju izlaze prethodnog sloja. Broj slojeva mreže određuje njenu dubinu. Termin "duboko učenje" nastao je baš iz ove terminologije. Svi slojevi koji svoje izlaze prosleđuju narednom sloju nazivaju se skrivenim slojevima.  Mreže sa više od jednog skrivenog sloja nazivaju se dubokim neuronskim mrežama. 


\par
Svaki neuron opisuje se pomoću vektora $w = (w_0, ..., w_n)$ koji se naziva vektorom težina. Ulazni parametrar $x = (x_1, ..., x_n)$ linearno se transformiše na sledeći način:
\begin{center}
	$f_w(x) = w_0 + \sum_{i=1}^{n} x_nw_n $
\end{center}
a zatim se primenjuje takozvana aktivaciona funkcija, $g$. Izlaz iz neurona je $g(f_w(x))$ i, uprkos linearnosti prve transformacije, izlaz ne mora biti linearna transformacija ulaza, tj. $g$ nije linearna funkcija. Za $g_i$ bira se nelinearna funkcija jer se u suprotnom dobija linearna funkcija; na ovaj način, mreža bi predstavljala linearnu funkciju i ne bi bilo moguće njom aproksimirati nelinearne funkcije dovoljno dobro.  Vrednost $w_0$ naziva se slobodnim članom. \\

[SLIKA NEURONA]

Model se formalno definiše na sledeći način:
\begin{center}
	$ h_0 = x $  \\
	$ h_i = g_i(W_ih_{i-1} + w_{i0})$, za $i=1, ..., L$
\end{center}
gde je $x$ vektor ulaza u mrežu predstavljen kao kolona, $W_i$ je matrica čija $j$-ta vrsta predstavlja vektor težina $j$-tog neurona u sloju $i$ a $w_{i0}$ je kolona slobodnih članova svih jedinica u sloju $i$. Funkcije $g_i$ su nelinearna aktivaciona funkcija i za vektor $t=(t_1, ..., t_n)$, $g_i(t)$ predstavlja kolonu $(g_i(t_1), ..., g_i(t_n))^T$. Na ovaj način dobija se funkcija čiji su parametri $W_i$ i $w_{i0}$ za $i=1,...,L$. Ako se parametri označe sa $w$, tada se model zapisati kao $f_w$. Parametri $w$ mogu se pronaći matematičkom optimizacijom nekog kriterijuma kvaliteta modela. Taj proces opisan je u delu ~\ref{subsec:optimizacija}.

\subsection{Aktivacione funkcije}


Preteča neuronskih mreža, perceptron, je model koji se sastoji samo iz jednog neurona čija je aktivaciona funkcija data sledećim izrazom:
\[ g(x)=
\begin{cases}
	1, 	& \text{~ako~} x \geq 0 \\
	0, 	& \text{~inače}
\end{cases}
\]

Definicija aktivacione funkcije perceptrona znači da njegova primena ima relativno jako ograničenje. S obzirom na to da će ulaz u funkciju $g$ biti linearna kombinacija ulaza i parametara, perceptron će moći da napravi podelu prostora određujući hiperravan. Ukoliko skup ulaznih podataka nije moguće podeliti linearnom funkcijom, ovakvo ponašanje nije zadovoljavajuće.

\par
Najčešće korišćene aktivacione funkcije su:
\begin{itemize}
	\item Sigmoidna funkcija: $\sigma (x) = \cfrac{1}{1+e^{-x}}$
	\item Tangens hiperbolički: $tanh(x) = \cfrac{e^{2x}-1}{e^{2x}+1}$
	\item Ispravljena linearna jedinica: $ReLU(x) = max(0, x)$
\end{itemize}

Sigmoidna funkcija bila je najkorišćenija aktivaciona funkcija pri radu sa neuronskim mrežama. Ograničena je (sve slike nalaze se u intervalu $(-1, 1)$), monotona i diferencijabilna u svakoj tački skupa $\mathbb{R}$, što je izuzetno bitno za optimizaciju. Međutim, što se argument više udaljava od nule, to nagib funkcije postaje manji. To znači da će gradijent funkcije biti mali i da će učenje teći jako sporo. 
Tangens hiperbolički srodan je sigmoidnoj funkciji ($tanh(x) = 2\sigma (x) - 1$) ali je imala veći uspeh od sigmoidne funkcije.  U okolini nule, ova funkcija slična je identičkoj, što olakšava učenje. Međutim, i pri korišćenju ove funkcije može se naići na problem sa malim gradijentima ukoliko se argument dovoljno udalji od nule. [Postoje načini da se problemi sigmoidne funkcije i tangensa hiperboličkog izbegnu.] -- ILI OBRISI ILI PREFORMULISI/OBJASNI
Uprkos tome što za razliku od prethodne dve funkcije nije ni ograničena ni diferencijabilna u svim tačkama domena, danas je ispravljena linearna jedinica najpopularniji izbor za aktivacionu funkciju. Funkcija je jednaka identitetu desno od nule i stoga se gradijent ne menja. Takođe, verovatnoća da se traži gradijent u tački u kojoj funkcija nije diferencijabilna je mala. Ipak, ni ova funkcija nije bez mana; problem često pravi deo desno od nule, gde je funkcija konstantna. To znači da je gradijent nula i da se prilikom optimizacije težine neurona neće izmeniti. Zbog nedostatka promene, može se desiti da neki neuroni u mreži postanu pasivni, tj. da im izlaz postane $0$. Za ovaj problem takođe postoje rešenja; jedno jeste da izlaz funkcije desno od nule ne bude konstanta $0$ već $\alpha x$, za neko malo $\alpha$. Ta modifikovana ReLU funkcija naziva se nakošena ispravljena linearna jedinica (eng. leaky rectified linear unit). 
\par
Iako sve ove funkcije imaju prednosti i mane u odnosu na preostale, ne postoji jedinstveni izbor nego je na osnovu problema neophodno zaključiti koju je aktivacionu funkciju najbolje koristiti.

\subsubsection{Izlazni sloj}

Neuronske mreže koriste se pri regresiji, određivanju funkcije koja opisuje vezu izmedju ulaza i izlaza, i klasifikaciji, svrstavanje ulaznih vektora u jednu od konačnog broja kategorija. \par

Pri regresiji, u poslednjem sloju ne primenjuje se aktivaciona funkcija. Proces optimizacije svodi se na minimizaciju funkcije greške. Kod rešavanja problema klasifikacije, koristi se funkcija mekog maksimuma (eng. softmax):
\begin{center}
	$ softmax(x) = \bigg( \cfrac{e^{x_1}}{\sum_{i=1}^{N} e^{x_i}}, ~...~, \cfrac{e^{x_N}}{\sum_{i=1}^{N} e^{x_i}} \bigg) $, 
\end{center}
gde je $N$ broj kategorija. Suma ovako dobijenog vektora je $1$ i stoga može predstavljati diskretnu raspodelu verovatnoća. Za vrednost aproksimacije uzima se kategorija kojoj odgovara najviša vrednost izlaznog vektora. Za optimizaciju pri radu sa probabilističkim problemima, kao što je problem klasifikacije, primenjuje se metod maksimalne verodostojnosti (eng. maximum likelihood estimate), odnosno traži se maksimum sledećeg izraza: 
\begin{center}
	$P(y_1, ..., y_N | x_1, ..., x_N)$.
\end{center}

\subsection{Optimizacija}
\label{subsec:optimizacija}

Ukoliko je neuronska mreža predstavljena kao funkcija $f_w$, gde su $w$ parametri mreže, neophodno je izvršiti minimizaciju ili maksimizaciju funkcije koja predstavlja kriterijum kvaliteta aproksimacije. Problem optimizacije u slučaju neuronskih mreža težak je zbog nekonveksnosti. Ona čini neke metode teško primenljivim ili izuzetno sporim. Moguće je i završiti u lokalnom optimumu funkcije. Uobičajeno se koriste metodi zasnovani na gradijentu funkcije. Postoje metodi drugog reda, zasnovani na hesijanu \footnote{Hesijan je matrica parcijalnih izvoda drugog reda.} ali je njegovo računanje u slučaju većeg broja parametara preskupo.

\subsubsection{Metod gradijentnog spusta}

Gradijent funkcije $f:\mathbb{R}^n \rightarrow \mathbb{R}$ u tački $x=(x_1, ..., x_n)$ ozačava se sa $\nabla f$ i predstavlja vektor parcijalnih izvoda u toj tački:
\begin{center}
	$\nabla f(x) = \bigg( \cfrac{\partial f}{\partial x_1}(x),...,\cfrac{\partial f}{\partial x_n}(x) \bigg)$ .
\end{center}

Gradijent funkcije u tački $x$ predstavlja pravac i smer najbržeg rasta funkcije pa $- \nabla f(x)$ predstavlja pravac smer najbržeg opadanja funkcije. 
\par

Metod gradijentnog spusta jedan je od najstarijih metoda  optimizacije. Iterativnim pristupom minimizuje se konveksna diferencijabilna funkcija. Polazeći od nasumično odabrane tačke i prateći pravac i smer gradijenta u svakom koraku, dolazi se do minimuma funkcije. Iterativni korak definisan je na sledeći način:
\begin{center}
	$ x_{i+1} = x_k + \alpha_i \nabla f(x_i), i=0, 1, 2, ... $,
\end{center}
gde je $x_0$ ta nasumično odabrana početna tačka a $\alpha_i$ je pozitivan realan broj koji se naziva veličinom koraka. Bitno je pažljivo odabrati veličinu koraka jer ova vrednost može uticati na divergenciju. Jedan primer odabira veličine koraka jeste niz za koji važe sledeći uslovi, pod nazivom Robins Monroovi uslovi \footnote{\url{https://en.wikipedia.org/wiki/Stochastic_approximation}} :

\begin{center}
	$$ \sum_{i=0}^{\infty} \alpha_i = \infty \hspace{2cm} \sum_{i=0}^{\infty} \alpha_i^2 < \infty $$ 
\end{center}

Naravno, postoje i drugi pristupi koji osiguravaju konvergenciju.
\par
Postavlja se i pitanje koliko koraka načiniti pre zaustavljanja. U praksi se koristi nekoliko kriterijuma. Jedan jeste zaustavljanje kada su dve uzastopne vrednosti $x_i$ i $x_{i+1}$ dovoljno bliske, kada su vrednosti funkcije za te dve vrednosti dovoljno bliske ali se može zaustaviti i nakon unapred određenog broja koraka. Postoji još kriterijuma i moguće ih je kombinovati.
\par
Iako jednostavan i široko primenljiv metod optimizacije, gradijentni spust nije najbolji izbor. Naime, pravac najbržeg uspona funkcije nije uvek i pravac koji osigurava najbrže približavanje optimumu funkcije. U praksi, gradijentni spust proizvodi cik-cak kretanje koje dovodi do spore konvergencije.

%[STOHASTICKI SPUST] \\
%[BACKPROP] \\ 
%[ADAM/RMSPROP?] -- bar procitaj\\ 



[MOZDA O TEOREMI O UNIVERZALNOJ APROKSIMACIJI?]

[SLIKA NEKE MREZE]



[AKTIVACIONE FUNKCIJE]

Uvod  (prosiriti?) \\
Vrste NN \\
Aktivacione funkcije \\
Perceptron / neuron (da li?) \\
Gradijentni spust, Propagacija unazad \\
Primene i ograničenja \\
Nešto o normalizaciji??? Regularizaciji